#  第三章 线性模型

线性模型是最基础的统计学模型之一，它在计算机被发明之前就被统计学家提出，不过它在现代的计算数学和统计学中同样有着重要的作用。线性模型能够清晰和直观地呈现输入数据与输出数据之间的关联，以及输入是如何影响输出的。有时候，在特定的回归任务和预测任务中，线性模型能比某些非线性模型更优，尤其是在小样本预测和稀疏样本或者低信噪比的情形中。

同样，线性模型在接收输入数据的时候，可以将输入数据进行变换（transformation），显著提升分类和回归能力，这样的方法称为基函数方法（basis function method）。线性模型假设了最终习得的 $Y=f(X)$ 函数是关于输入 $X^T=(x_1,x_2,...,x_p)$ 每个坐标线性组合的形式，表示成矩阵即
$$
Y=\beta_0+\beta^TX+\epsilon   \ \  (3.1)
$$


其中 $\beta=(\beta_1;\beta_2,...,\beta_p)$ 且 $\beta_0$ 为偏置项（bias），$\epsilon$ 为可以接收的误差。

> 两种表示模型的形式，什么时候需要加 帽号 和 epsilon

在本章线性模型的讲解中，我们主要介绍线性回归和线性分类模型。同时，我们需要对简单的模型进行拓展，因为很多非线性模型方法在本质上是对线性模型方法的扩展，这能够很好地帮助我们建立一个分析基础从而更快速和轻松地理解之后的模型。

## 线性回归

线性回归假设了：对于一个输入矩阵 $X^T=(X_1,X_2,...,X_p)$，其中 $X_k$ 表示所有样本第 $k$ 分量组成的向量，回归函数 $E(Y|X)$ 是线性的。线性回归模型的两个基本的用途是：一利用已经建立的回归模型对一组自变量（independent value）样本（即 X）进行预测，可以估计因变量（即 Y）的值；二利用模型进行自变量和因变量之间相关性的分析，例如检测是否存在线性关系或者检测是否存在X数据冗余。

### 线性回归模型推导

我们主要的任务是利用回归模型来完成数据预测任务，下面我们来推导线性回归模型。

根据线性回归模型的假设：回归函数 $E(Y|X)$ 是线性的（为什么是条件概率的形式？我们在统计解释中说明），也就是说我们需要习得的 $f$ 有一般形式
$$
f(x )=\beta_0+\sum_{j=1}^p{x_j \beta_j}                       	\ \          (3.2)
$$
通常我们假设有一组数据集 $\chi=\{{X^{(i)}}^T=(x_1^{(i)},x_2^{(i)},...,x_p^{(i)})\}_{i=1}^N$ 和 $Y=\{y^{(i)}\}_{i=1}^N$ ，同时 $X^{(i)}$ 与 $y^{(i)}$ 对应，我们的目标是计算出 $\beta=(\beta_1,\beta_2,...,\beta_p)$ 和偏置 $\beta_0$ 项。那么由此根据 (3.2) 可以对任意的输入 $x$ 得对 $y=f(x)$ 的值进行预测。

那么如何计算 $\beta$ 呢？我们利用已有的数据集 $(\chi,Y)$ 进行学习的过程就是计算 $\beta$ 的过程。根据 (3.2) 对于任意 $X_k^T\in \chi$ 需要满足 
$$
y^{(k)}=f({X^{(k)}}^T)+\epsilon_k \ \ (3.3)
$$
并且误差 $\epsilon_k​$ 必须非常小才能让我们接受它。 也就是对于输入集合 $\chi​$ 中每一组数据，我们所计算出的 $\beta​$ 必须使我们得到的模型对所有数据都有非常小而可容忍的误差。

最普遍常用的最小化误差的方式是均方误差（least square），这里给出均方损失函数
$$
R(\beta)=\sum_{j=1}^N{(y^{(j)}-f({X^{(j)}}^T))^2}
$$

$$
R(\beta)=\sum_{j=1}^N{(y^{(j)}-\beta_0-\sum_{i=1}^p{X_i^{(j)} \beta_i})^2} \ \ (3.4)
$$

(3.4) 中均方损失 $R(\beta)​$ 的大小就决定了误差的大小，因此我们要最小化 $R(\beta)​$。为了数学上的便利，我们令
$$
X_{N\times (p+1)}=        \begin{pmatrix}
        1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_p^{(1)}\\
        1 & x_1^{(2)}& x_2^{(2)} & \cdots & x_p^{(2)} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
       1 & x_1^{(N)} & x_2^{(N)} & \cdots & x_p^{(N)} \\
        \end{pmatrix}
$$

$$
\beta^T_{1\times (p+1)}=(\beta_0,\beta_1,...,\beta_p)
$$

上 (3.2) 改写成
$$
f(X)=X\beta
$$
可以看出来我们只是将偏置项写到了矩阵和向量中，那么经过变换后
$$
R(\beta)=(y-X\beta)^T(y-X\beta) \ \ (3.5)
$$
目标函数
$$
\hat{\beta}=\arg_{\beta} \min{(y-X\beta)^T(y-X\beta)} \ \ (3.6)
$$
可以看出 $R(\beta)$ 是连续单值函数，因此直接另一阶导数为零
$$
\frac{\partial{R(\beta)}}{\partial{\beta}}=0 \implies -2X^T(y-X\beta)=0 \ \ (3.7)
$$

$$
\frac{\partial^2{R(\beta)}}{\partial{\beta^2}}=2X^TX>0 \  \ (3.8)
$$

根据 (3.7) 可知 $R(\beta)$ 存在二阶导数，再由 (3.8) 可得根据 (3.7) 得到的 $\beta $ 使 $R(\beta)$ 取到了最小值。其中 (3.7) 涉及到了单值函数对向量求导，具体求导法则参见【附录】。

下面假设矩阵 $X$ 是列满秩的，因此 $X^TX$ 可逆，根据 (3.7) 得到
$$
\hat{\beta}=(X^TX)^{-1}X^Ty \ \ (3.9)
$$
当然很多情况下，矩阵 $X$ 不是满秩的，因为存在某些冗余数据或者某些行向量存在线性相关性（如 $X_2=3X_1$ ），那么这样的话，我们计算出来的 $\beta$ 就不是唯一的了。这里给出三种解决方案

1. 进行数据预处理，特征过滤
2. 进行正则化处理，比如说岭回归（ridge regression）之后会提及
3. 直接去掉冗余的行向量

当然，很多的库函数已经帮我们解决了这个问题，他们会自动帮我们处理。最终我们习得的 $\hat{y}$
$$
\hat{y}=X(X^TX)^{-1}X^Ty \ \ (3.10)
$$

同时，我们可以将我们的线性回归模型进一步推广，模型的任意一个输入样本 $X^{(i)}_j$ 可以是来源于

1. 样本 $X^{(i)}_j$ 经过函数变换后的值，比如说经过 $sin$ 函数变换后 $sin{X^{(i)}_j}$ 的值作为新的 $X^{(i)}_j$
2. 也可以是别的样本的组合形式，比如 $X^{(i)}_j=X^{(i)}_1\cdot X^{(i)}_2$ ，或者 $X^{(i)}_j={X_j^{(i)}}^2$，那么很明显我们也可以用线性回归模型来做多项式拟合

### 最小二乘法的统计解释

我们的模型训练数据 $(X,Y)$ 实质上是从分布 $D_X,D_Y(X,\beta)$ 上取下的样本。需要注意的是这里的 $X$ 与一般的统计模型假设不同：$X$ 本身也是随机变量，是从一个分布 $D_X$ 上获取的，因此任意一个 $y^{(j)}$ 实质上是有条件依赖于 $X^{(j)}$ 的，不过即使 $y^{(j)}$ 条件独立，最小二乘法仍然成立，因为最小二乘法只是代表了残差，对 $X$ 本身并无关系。

但是为了从统计角度进行分析，我们需要给出假定：观测变量 $y^{(j)}$ 与给定  $X^{(j)}$不相关，并且有定方差 $\sigma^2$ ，同时保证 $X^{(j)}$ 不是随机而是固定的。

根据 (3.1) 对模型的 $N$ 次观察取样得到
$$
y^{(j)}=\beta_0+X^{(j)} \beta +\epsilon_j \\
1\leq j \leq N
$$
这里的 $y_j$ 是真实的输出值即样本中的值，即有 $y^{(j)}-\hat{y^{(j)}}=\epsilon_j$，我们需要对模型增加假定：这里的 $\{\epsilon_j\}_{1\leq j\leq N}$ 是随机误差，是无法得知的，因此假设满足正态分布 $N(0,\sigma^2)$ 条件 :
$$
E(\epsilon_j)=0 \\Var(\epsilon_j)=\sigma^2\\1\leq j\leq N
$$
同时 $\{\epsilon_j\}_{1\leq j\leq N}$ 中的每一个 $\epsilon_k$ 都独立同分布，这也就说明残差 $\hat{y^{(j)}}-y^{(j)}$ 满足相同的正态分布 $N(0,\sigma^2)$ ，结合 (3.1) 和 (3.3) 就是我们的模型 $f$ 。

根据之前的假定我们能够得出 $\hat{\beta}$ 的方差
$$
Var(\hat{\beta})=Var( \ (X^TX)^{-1}X^Ty \ ) \\ \tag{3.11}
$$

$$
=||(X^TX)^{-1}X^T||^2Var(y)\\=||(X^TX)^{-1}||^2 \cdot ||X^T||^2\cdot \sigma^2\\=(X^TX)^{-1}\sigma^2
$$

那么 $\hat{\beta}$ 的分布也就是 $\hat{\beta} \text{~} N(\beta,(X^TX)^{-1}\sigma^2)$ ，但是我们想要知道 $\hat{\beta}$ 的精确度还需要知道 $\sigma$ 的值，于是我们需要进一步估计 $\sigma^2$ ，利用我们之前计算的残差有
$$
\hat{\sigma}^2=\frac{1}{N-p-1}\sum_{i=1}^{N}{(\hat{y^{(j)}}-y^{(j)})^2}
$$
求和前的系数是为了保证 $E(\hat{\sigma}^2)=0$ ，即保证 $\hat{\sigma}^2$ 是 $\sigma^2$ 的一个无偏估计。这里的残差 $(\hat{y^{(j)}}-y^{(j)})^2$ 满足 $N(0,\sigma^2)$ 分布，那么也可以另外推出
$$
\hat{\sigma}^2 \text{~}\frac{\sigma^2}{N-p-1}\chi_{N-p-1}^2
$$
其中 $\chi_{N-p-1}^2$ 为自由度为 $N-p-1$ 的卡方分布。于是结合 (3.11) 得到 $\hat{\beta}$ 的分布，接下来通过上面decubitus的估计式，我们需要求出 $\hat{\beta_j}$ 的置信区间，首先建立一个假设 $H:\beta_j=0$

然后我们构造一个标准化的变量
$$
z_j=\frac{\hat{\beta_j}}{\sqrt{\hat{\sigma}^2d_j}} \tag{3.12}
$$
其中，$d_j$ 为矩阵 $(X^TX)^{-1}$ 对角线上第 $j$ 个元素，(3.12) 在假设 $H$ 下满足自由度为 $N-p-1$ 的 $t_{N-p-1}$ 分布



> 置信区间分析 待写

### 实例3.1 拟合函数

我们来运用最小二乘法线性回归算法来拟合一个函数$Z=X_1+X_2^2+X_1X_2$ ，对于这样一个函数，我们首先需要做一个变换令 $Y_1=X_1$，$Y_2=X_2^2$，$Y_3=X_1 X_2$，转化成一个线性回归问题 $Z=Y_1+Y_2+Y_3$

> 这里需要更详细的解释 后补

先给出我们所需要的库

```python
# 引入 基本科学运算库
import numpy as np
# python 作图库
import matplotlib.pyplot as plt
# 三维作图库
from mpl_toolkits.mplot3d import Axes3D as aa
# 引入 sklearn.linear_model
from sklearn import linear_model as limd 
# 引入 交叉验证函数
from sklearn.model_selection import train_test_split
```

**第一步：处理数据**

随机产生100组数据，同时加入随机偏移噪声

```python
''' 模型数据处理
'''
scale=2 # 坐标数据尺度
# 随机产生二维数据
x1=np.random.rand(100,1)*scale 
x2=np.random.rand(100,1)*scale
X=np.ones([100,3]) # 模型的输入数据
# 矩阵表示 x1+x2^2+x1*x2
X[:,0]=x1[:,0]
X[:,1]=x2[:,0]*x2[:,0]
X[:,2]=np.multiply(x1[:,0],x2[:,0])
# 产生随机误差
bias=(np.random.rand(100,1)-0.5 )*0.4*scale # 偏移系数 0.4
Z=X[:,0]+X[:,1]+bias[:,0]+X[:,2]
```

**第二步：拟合**

调用 sklearn.linear_model 最小二乘法的线性回归函数进行函数拟合。

这是我们模型的核心，这里简单介绍一下过程，limd 是 linear_model 对象，运用的方法是 LinearRegression()，fit(X,Y) 函数是对数据进行拟合，拟合的结果可以从 lr 对象的属性得知，其中 coef_ 表示线性模型中的系数 $\beta$ ，最后输出验证结果。lr.score(X,Y) 是对数据 $X,Y$ 拟合结果的评分。

我们接着介绍 **LinearRegression()** 方法的参数：

**fit_intercept** : boolean, 默认 True

> 模型是否使用截距

**normalize** : boolean, 默认 False

> 当 `fit_intercept` 为 False，该参数无意义，自动忽略。如果被设置为 True ，自动进行正则化 `L2`

**copy_X** : boolean, 默认 True

> X 是否能被修改，True 表示能；False 表示不能

**n_jobs** : int, 默认 1

> 计算任务数，达到一定数会自动开启加速优化。

```python
# 加载最小二乘法回归函数
lr=limd.LinearRegression()
print "拟合结果\n",lr.fit(Xt,Zt) # 数据进行拟合
print "系数\n",lr.coef_
print "验证结果\n",lr.score(Xtest,Ztest) # 用验证数据进行模型评分
```

然后给出这段代码的输出结果

```python
随机曲面拟合
拟合结果
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
系数
[ 1.07534516  1.05496164  0.9445371 ]
验证结果
0.992572204065
```

**第三步：作图**

1. 在拟合函数的定义域范围内制作一个二维网格 [XLR,YLR]
2. 然后通过两层循环，计算网格上每一点模型的拟合值 ZLR(i,j)，通过 Axes3D.plot_surface() 把网格上模型拟合值代表的曲面画出来
3. 分别作出原始数据的散点图（红），以及模型拟合后数据的散点图（绿）
4. 设置图的大小和相关 label 并显示

```python
# pyplot 三维图片加载
fig=plt.figure()
# ax 为三维图对象
ax=aa(fig)

# plot_surface 部分
# 在数据范围内生成 meshgrid 网格坐标 100*100
Xlr=np.linspace(np.min(x1[:,0]),np.max(x1[:,0]),100)
Ylr=np.linspace(np.min(x2[:,0]),np.max(x2[:,0]),100)
# 两个一维向量通过 meshgrid() 生成二维网格
[XLR,YLR]=np.meshgrid(Xlr,Ylr)
# 在网格上计算模型的输出值
i=0
ZLR=np.ones([100,100]) 
for x in Xlr:
    j=0
    for y in Ylr:
        # 对每一个坐标点作为模型的输入求解输出值 ZLR[i,j]
        ZLR[i,j]=np.dot([x,y*y,x*y],lr.coef_)+lr.intercept_
        j=j+1
    i=i+1
    
ax.plot_surface(XLR,YLR,ZLR,cmap='winter')  # 绘制模型输出矩阵的曲面
Zlr=np.dot(X,lr.coef_)+lr.intercept_ # 根据矩阵X计算模型输出
ax.scatter(x1,x2,Z,c='r') # 根据数据的三维散点图
ax.scatter(x1,x2,Zlr,c='g') # 根据模型的三维散点图
# 设置视觉范围 仰角45 旋转-60
ax.view_init(elev=45,azim=-60)
# 设置 x y z 取值范围
ax.set_xlim([np.min(X[:,0]),np.max(X[:,0])])
ax.set_ylim([np.min(X[:,1]),np.max(X[:,1])])
ax.set_zlim([np.min(Z),np.max(Z)])
# 设置坐标 label
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('z')
# 展示
plt.show()
```





![多项式平面拟合1](C:\Users\lenovo\Desktop\写作\机器学习\3\多项式平面拟合1.png)

![多项式平面拟合2](C:\Users\lenovo\Desktop\写作\机器学习\3\多项式平面拟合2.png)



图片说明：对于我们拟合的曲面，以及原数据散点（红），拟合数据散点（绿），从图中可以看出，所有数据点都位于平面两侧不太远的地方。可以看出我们的拟合精度还是可以接受的，随机偏移噪声的影响对我们最终的模型结果影响并不是非常大。

### 梯度下降方法	

在数学中，梯度代表的是一个函数的最快上升方向，比如给定一个定义在 $\R^2$ 上的单值连续光滑函数 $f$ ，想要知道它在某点 $ (x_0,y_0)$ 的增加量最快的方向就是
$$
\nabla {f}=\frac{\part{f}}{\part{x}}\hat{i} + \frac{\part{f}}{\part{y}}\hat{j} + \frac{\part{f}}{\part{z}}\hat{k}
$$
其中 $\hat{i},\hat{j},\hat{k}$ 是沿 $x,y,z$ 轴的单位向量，那么梯度向量就是 $(\frac{\part f}{\part x},\frac{\part f}{\part y},\frac{\part f}{\part z})$.



那么我们如何利用梯度向量求解最优化问题呢？显然，我们写出一个目标函数，就是要寻找目标函数在约束条件下的极值点。

先给出梯度下降方法的基本思想：设定一组关于 $\beta$ 参数的初始值，以及学习率，然后通过根据梯度和学习率，不断迭代和修改 $\beta$ 值，使得最终的目标函数达到极值，这个过程就是梯度下降。例如（3.6）是一个凸函数最小值问题，以此为例，我们想要求解出使得 $R(\beta)$ 最小的 $\hat{\beta}$ 值，可以通过不断迭代
$$
\beta^{t+1}=\beta^t-\alpha \frac{\part R}{\part \beta} \tag{3.13}
$$
其中 $t$ 表示迭代次数，$\alpha$ 是学习率表示单次迭代的下降快慢程度，这里 $\beta$ 是一个向量。化解成展开形式为
$$
\beta^{t+1}_j=\beta^t_j-\alpha \sum_{i=1}^N{(f(X^{(i)})-y^{(i)})X^{(i)}_j} \tag{3.14}
$$
因为有
$$
\frac{\part R(\beta)}{\part \beta}=\frac{\part}{\part \beta}\sum_{i=1}^N\frac{1}{2}{(y^{(i)}-\sum_{j=1}^pX_j^{(i)}\beta_j)^2}\\
=[-\sum_{i=1}^N{	(y^{(i)}-\sum_{j=1}^p{X_j^{(i)}\beta_j})X_k^{(i)}	}]_k \tag{3.15}
$$
最后结果是一个与 $\beta$ 适合的向量，$\frac{\part R(\beta)}{\part \beta_k}$表示分量 $\beta_k$ 的梯度大小。

>  可以看到基本原理和爬坡的过程非常相似。

这里给出梯度下降算法框架：

1. **确定学习参数 $\alpha$**

$\alpha$ 的值应该适中，如果过大就会不收敛或发生震荡现象；如果过小收敛会非常慢，浪费计算资源。

2. **设定初始值 $\beta^0$**

让 $\beta$ 从 $\beta^0$，开始迭代。

3. **迭代计算 $\beta^{t+1}$**

如果没有达到收敛，迭代将继续下去。

4. **判断是否收敛**

当 $R(\beta)$ 的值小到一个我们可以接受的误差 $e$ 的时候，我们就停止计算；否则继续3迭代。

最终求解出来的 $\beta^k$ 就是最终的最优参数值。 

> 代码 图片

### Lasso方法与Ridge方法

Lasso方法和岭回归（Ridge）方法属于收缩方法（shrinkage method），他们的基本思想是相近的：通过在目标函数增加一个关于系数的代价，使得在求解过程中系数也会逐渐收缩趋近于零。使用收缩方法有两个好处：其一能够有效避免模型的过拟合问题，即模型的训练得和数据过于一致以至于出现了泛化程度不高的现象。其二能够避免 $X$ 不是满秩的问题，前面说过（3.9）需要满足 $X$ 为列满秩的假设才能够使得 $X^TX$ 是可逆的，因此即使最小二乘法因为数据原因不能进行下去的情况下，收缩方法有效。

**Ridge方法**

岭回归是这样定义目标函数的
$$
\hat{\beta}=\arg_{\beta}\min\{\sum_{j=1}^N{(y^{(j)}-\beta_0-\sum_{i=1}^p{X_i^{(j)} \beta_i})^2 +\lambda\sum_{i=1}^p{\beta_j^2}}\}\tag{3.13}
$$

其中正则化参数 $\lambda\geq 0$ 表示收缩程度，$\lambda$ 越大，收缩程度越大。如果在模型中有很多的线性相关变量，那么模型的系数 $\beta$ 的值是不稳定的，即存在多个 $\beta$ 满足（3.7），并且 $\beta$ 具有很大的方差。不过在岭回归方法中，这种情况得到缓解，因为伴随着惩罚项的收缩，那些相关项的系数可能会形成正负相消，具体说在一个变量上的一个很大的正系数可能被在其相关变量上的类似大小的负系数抵消，从而不对最终求解出的 $\beta$ 值产生影响。选取大小合适的 $\lambda$ 参数值是非常重要的一点，若 $\lambda$ 过大，导致 $||\beta||^2$ 过小，欠拟合；若 $\lambda$ 过小，导致 $||\beta||^2$ 过大，过拟合，因此 $\lambda$ 值应该适中。

同时我们还应该注意到，如果想要分析方差，岭回归模型中对输入样本 $X$ 的要求：对于每一个样本必须是经过标准化的，因为随着 $X$ 的缩放，最终求解方差的结果会有很大差别。

> centering 待写 equivariant

同样将 $X$ 第一列用 $\bf{1}$ 向量代替，$X$ 为 $N \times (p+1)$ 矩阵。用矩阵的形式表示岭回归目标函数
$$
R(\lambda,\beta)=(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta \tag{3.14}
$$
按照之前说的线性回归最小二乘法偏导求解的方程得到
$$
\hat{\beta}^r=(X^TX+\lambda I)^{-1}X^Ty \tag{3.15}
$$
可以注意到 $X^TX+\lambda I$ 一定是可逆的，因此 $\hat{\beta}^r$ 可直接求解。这就是岭回归模型的优势之一：即使因为某些原因 $X^TX$ 不列满秩，（3.15）仍然成立。若 $X$ 是正交矩阵，那么岭回归和最小二乘法的解相差一个倍数，即 $\hat{\beta}^r(1+\lambda)=\hat{\beta}$ 。

> 梯度下降法公式 岭迹图
>
> 方差分析 主成分分析

**Lasso方法**

Lasso方法在很大程度上和岭回归是类似的，都是利用正则化项进行惩罚。只不过Ridge法用$L_2$范数，Lasso法用$L_1$范数，在本质和目的上没有什么不同。

Lasso方法的拉格朗日形式为
$$
\hat{\beta}^l=\arg_{\beta}\min{\sum_{i=1}^N{(y_i-\beta_0-\sum_{j=1}^p{x^{(i)}_j\beta_j})^2+\sum_{j=1}^p{\lambda|\beta_j|}}}    \tag{3.16}
$$
可以看到（3.16）和（3.13）非常相似，除了最后的惩罚项都是相同的。（3.16）是一个二次规划问题，我们也可以采用软件自带的优化包解决，同时数值优化方法还有坐标下降算法，最小角回归算法，近端梯度下降算法等，原理部分都超出本书的范围，需要读者自行去网络上寻找资源。

> gradient decreasing method

> Lp范数的讨论

![Lp](C:\Users\lenovo\Desktop\Lp.jpg)

为了能够更好地理解 Lasso 和 Ridge 之间的联系，我们假设他们的变量 $X$ 都是二维向量。根据 Lasso 的约束条件有 $|X_1|+|X_2|\leq t$ 其中 $t\geq 0$ 为一常数，再根据 Ridge 约束条件有 $X_1^2+X_2\leq t$ 其中 $t\geq 0$ ，这就对应了（图）中的情形，同时有目标函数 $R(\beta)$ 的等值线，每一条等值线都代表了一个函数取值，其中中心 P 是函数的极小值，我们需要在令目标函数值尽可能得小和满足约束条件之间做一个折中，也就是正好取的是等值线和约束曲线相切的那个点，这就是我们的最优情况。

> 图片说明 beta

Lasso 方法和  Ridge 方法的不同之处在于正则化项。现在来讨论一般情况，对于一般情形的正则化项 $\sum_{j=1}^p{|\beta_j|^r}$ 对于不同的 $r$ 的取值，约束曲线的情况是不同的。我们可以从（图）中看出，当 $r\geq1$ 时，约束曲线是凸函数；当 $r \in (0,1)$ 时，约束曲线是凹的。当 $r \in (1,2)$ 时，约束代表的是 Lasso 与 Ridge 的折中。特殊地，当 $r=1$ 时，是令曲线满足凸性质的最小的 $r$ 的取值。当 $r \neq 1,2$ 时，方差分析不太方便，当 $r\in (0,1)$ 时，便不是一个凸优化的求解问题了，问题将变得更加复杂。因此常用的就是 Lasso 或 Ridge 的正则化项。

### 实例3.2 预测波士顿房价

这节我们会应用之前所学的线性回归方法来对 sklearn 中自带的数据集进行学习并且预测。

**需要用到的库**

```python
import numpy as np

# 作图
import matplotlib.pyplot as plt

#  线性模型
from sklearn import linear_model as limd 

# 数据集 波士顿房价
from sklearn.datasets import load_boston as lb

# 数据预处理
from sklearn import preprocessing as pp

# 数据归一化预处理
from sklearn.preprocessing import StandardScaler as ss

# 交叉验证
import sklearn.cross_validation as cv

# 矩阵计算
from sklearn import metrics
```



**加载数据**

预测房价需要分析哪些变量和房价的高低相关呢？Boston 数据集已经给出了13个特征，我们首先直接加载它。

```python
print "回归预测房价"
# 加载 boston 数据
boston=lb()
# 输出数据特征
print boston['DESCR']
```

我们加载完毕后，分析数据集 boston 的组成和结构:

```python
boston.keys()

Out[35]: ['data', 'feature_names', 'DESCR', 'target']
```

看出 boston 由4个主要键值，data 表示输入特征集；feature_names 表示特征名称；DESCR 表示数据集的描述，包括数据名称，特征单位以及数据值等；target 表示目标值，即训练的目标输出。

这里简单介绍一下 boston 数据集的描述（DESCR）信息：

```python
Data Set Characteristics:  
    # 实例数
    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive
    
    :Median Value (attribute 14) is usually the target
    # 特征信息
    :Attribute Information (in order):
        - CRIM     人均犯罪率
        - ZN       超过 25,000 平方英寸的住宅区占地比例
        - INDUS    非零售区占地比例
        - CHAS     是否靠近河边 （1 靠近；0 不靠近）
        - NOX      一氧化氮浓度
        - RM       每个居住房屋的平均房间数
        - AGE      在1940年之前建成且业主自主的房子的比例
        - DIS      距波士顿市中心的距离
        - RAD      周边高速大道的可到达指数
        - TAX      每 10,000 美元的财产税率
        - PTRATIO  师生比例
        - B        1000(Bk - 0.63)^2 Bk 为黑人比例
        - LSTAT    % 社会底层的比例
        - MEDV     房价值（目标变量）
```

#### 

#### Ridge 回归





控制台输出

```python
回归预测房价
['data', 'feature_names', 'DESCR', 'target']
模型
Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, random_state=None, solver='auto', tol=0.001)
训练拟合评分
 0.774
预测均方误差
 28.300
系数
 [-0.10420072  0.01474884  0.01109709  3.29285993 -7.86386651  4.51696735
 -0.01665711 -1.11167889  0.29372571 -0.01323551 -0.99963203  0.01010772
 -0.49263281] 
截距
27.3570597241
```

从输出结果分析，我们训练的 Ridge 模型并不完美，因为可以看到验证集的训练拟合评分只有0.774，同时预测均方误差有28.300之多，因此可以推测单纯的线性回归模型可能无法解决这样的问题。所以，我们需要另外想办法，是否能够通过自变量数据特征之间的组合、变换后作为新的自变量，然后再通过线性模型训练提高泛化能力和预测能力？

#### 模型优化

事实上，我们可以利用多项式特征变换解决这个问题：把自变量 $X=(x_1,x_2,...x_p)$ 利用多项式的想法拓展成 $X'=(x_1,x_2,...x_p,x_1^2,x_1x_2,x_1x_3,...x_p^2)$ 即将线性模型转换成了一个最高为二次的多项式模型，那么 $\beta$ 总共有 $(^p_2)+p+1$ 个未知的参数。 



```python
多项式回归预测房价
模型
Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, random_state=None, solver='auto', tol=0.001)
训练拟合评分
 0.937
预测均方误差
 17.000
系数
 [  0.00000000e+00  -3.89936462e-01  -4.87538997e-01   5.08679134e-02
   2.87410466e-01  -1.63111424e-02   5.66393955e-01   8.44987615e-01
   6.07480708e-01   1.54690940e+00  -2.41539630e-01  -8.14558400e-01
   2.86702975e-01  -1.07054133e+00   6.40090955e-05  -4.79791833e-02
   4.42453072e-01   2.37672412e+00  -1.26862645e+00   1.36894909e-01
  -5.70754016e-03  -4.68481571e-02   4.83421541e-01  -3.67209121e-02
   2.64272903e-01  -5.15202891e-04   2.64557986e-02   4.78746061e-04
  -3.18185939e-03  -3.86146676e-02   1.00658318e+00  -2.39990812e-02
  -2.60477571e-03  -1.95919257e-02   1.14699771e-02   9.89940832e-05
  -7.73933333e-03   1.12503167e-03  -6.32630125e-04   1.80752464e-02
  -4.94050764e-01   2.45084774e+00   1.43617633e-02   6.07630977e-03
   7.68412909e-02   8.40095210e-03   1.33806111e-03  -3.45348451e-02
  -4.25898781e-03  -5.54562243e-02   2.87410466e-01  -8.67903967e-01
  -1.76821863e+00   4.33367657e-02   9.60370774e-01  -6.40151350e-01
   9.92910310e-03   5.99444645e-01   3.12144425e-03  -4.37794699e-02
  -2.10536319e-01  -6.86387202e-01  -4.21012303e-01   3.13339659e+00
  -8.54920189e-01  -1.05180408e-01  -6.41916953e-01   4.38028044e-02
   2.30281944e+00   1.12752900e+00  -3.01039860e-02   3.44783892e-01
  -3.86804041e-01   1.06023489e-02  -2.44474066e-01  -8.94430049e-03
  -1.43210380e-01  -4.66478461e-05   1.29539329e-02   1.74945130e-02
  -5.62617934e-04  -1.00504983e-02  -7.03764883e-04  -5.09043206e-03
   4.88657896e-01  -1.95085901e-01  -5.63478638e-04  -1.14003008e-01
  -2.17944277e-02   8.99680766e-02  -1.70615053e-01   7.23160193e-03
   1.31705767e-01  -1.77171411e-03  -8.33276129e-02  -1.42630946e-05
   5.86313426e-03   2.20041205e-04   1.70866655e-03   7.96239919e-02
  -5.35703514e-03   5.07871263e-02  -7.50445589e-05  -1.58708090e-03
   1.84004046e-02] 
截距
-18.1240331127
```



## 线性分类



### 线性判别分析

线性判别分析（Linear Discriminant Analysis）是一种常用的线性分类算法。对于一个分类任务，基于这样的思想来完成样本的分类：假设训练样本$\{(x_i,y_i)\}_{1\leq i\leq N}$满足条件，$x_i \in \mathcal{X}\subset \R^d$为d维向量，被映射到类别集合$ \mathcal{Y} =\{1,2,...K\}$. 假设存在k维的超平面于样本空间之中，同时满足$k\leq d$，使得每个样本都被投影到这个超平面上去。对于被投影的样本需要满足同一个类的样本投影点尽可能地接近，而不同类的样本投影点尽可能地远离。然后在k维超平面上找到各个样本投影点之间的分界线，对于位置的样本同样采取投影的方法，根据投影点所落的区域来决定新样本类别的划分。

为了直观地体现LDA算法的思想，以一个简单的例子为引：有二分类问题 d=2，k=1，$\mathcal{Y}=\{-1,+1\}$.

![1540641802276](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1540641802276.png)

图？可以看出图一的lda分类器并不是完美的，因为两个类别之间的样本投影点存在很大的重叠部分，类别内的样本投影点不够紧凑，这就导致了对于未知样本分类的误差比较大；图二的lda分类器近似是完美的，因为样本投影点之间有明显的分界线，且类别内的样本投影点分布比较紧凑，这样的分类器可以更精确地预测新的样本。

#### LDA模型推导

下面从一个简单的二分类情形对线性判别分析进行推导，同时将二分类情形推广到多分类情形。

给定训练数据集$D=\{(x_i,y_i)\}_{1\leq i\leq N}$，存在一个二分类问题$ y_i \in\mathcal{Y}=\{-1,+1 \}$. 假设样本点都被投影到了直线$\omega$上，使得$\omega$为最优的投影直线。那么根据LDA算法的基本思想，首先，要使得同一类的样本投影点之间尽可能地紧凑，也就是说，同类样本投影点的协方差要尽可能地小；同时，对于不同类别的样本投影点之间要存在尽可能大的间隔，也就是说，不同类别的样本投影点中心的距离要最大化。

对于第一个条件，可以得到
$$
\min{(\omega^T\Sigma_- \omega+\omega^T \Sigma_+ \omega)}\tag{17}
$$
其中$\Sigma_-$为负样本的协方差矩阵；$\Sigma_+$为正样本的协方差矩阵。$\omega^T \Sigma \omega$将样本空间内的样本协方差变换为在直线$\omega $上的样本投影点协方差。对于两类样本类内的聚合程度要最小化。

对于第二个条件，可以得到
$$
\max{||\omega^T\mu_--\omega^T\mu_+||^2}\tag{18}
$$
其中$\mu_-$为负样本的均值向量；$\mu_+$为正样本的均值向量。$\omega^T\mu$将原样本空间内的均值向量同样变换到了直线$\omega$上。对于两类样本投影点的中心均值向量要尽可能地远离。

将两个条件结合起来考虑，结合式$(17)$和式$(18)$可以得到目标函数
$$
L=\frac{||\omega^T\mu_--\omega^T\mu_+||^2}{\omega^T\Sigma_- \omega+\omega^T \Sigma_+ \omega}\\
=\frac{\omega^T(\mu_--\mu_+)(\mu_--\mu_+)^T\omega}{\omega^T(\Sigma_-+^T \Sigma_+) \omega}\tag{19}
$$
因此最终的目标是最大化函数$L$. 

定义类内散度矩阵（within-class scatter matrix）为
$$
\begin{align}
M_\omega&=\Sigma_-+\Sigma_+\\
&=\sum_{\mathcal{x\in X_-}}{(x-\mu_-)(x-\mu_-)^T}+\sum_{\mathcal{x\in X_+}}{(x-\mu_+)(x-\mu_+)^T}
\end{align}\tag{20}
$$
其中$(x-\mu)(x-\mu)^T$是关于向量$x$的协方差矩阵。

定义类间散度矩阵（between-class scatter matrix）为
$$
M_b=(\mu_--\mu_+)(\mu_--\mu_+)^T\tag{21}
$$
式$(21)$其实就是关于两类样本点的均值向量的协方差矩阵。

结合式$(19)(20)(21)$就是需要求解的LDA模型。但是可以看出光这样还不能够对$\omega$进行求解，因为$\omega$可以在保持方向不变的情况下有任意多个长度使得$(19)$被满足。因此，需要额外增加限制条件转化为
$$
\min_{\omega}{-\omega^T(\mu_--\mu_+)(\mu_--\mu_+)^T\omega }\\
s.t. \; \omega^T M_\omega \omega =1\tag{22}
$$
式$(22)$是个二次优化问题，可以利用拉格朗日乘子法进行求解
$$
L(\omega)=-\omega^TM_b\omega +\lambda(\omega^T M_\omega \omega - 1)\\
\lambda \geq 0\tag{23}
$$
然后对该式对$ \omega $进行求偏导
$$
\frac{\part L(\omega)}{\part \omega}=0 \\
\Rightarrow M_b\omega=\lambda M_\omega\omega \tag{24}
$$
同时注意到一个特殊的现象，$M_b \omega $事实上是固定方向的$(\mu_--\mu_+)$，因为$(\mu_--\mu_+)^T\omega$是一个标量。于是，不妨设$M_b\omega =\lambda(\mu_--\mu_+)$，并将其带入式$(24)$最终得到
$$
\omega=M_{\omega}^{-1}(\mu_--\mu_+)\tag{25}
$$
从最后得到的模型结果来看，我们只要求出原始二类样本的均值和方差就可以确定最佳的投影方向$ \omega$了。

#### 贝叶斯观点下的LDA

接下来从贝叶斯理论来深入理解LDA的思想，分析一个简单的原始样本空间内的LDA分类器。将分类任务进一步推广为多分类问题：对于训练数据集$\{(x_i,y_i)\}_{1 \leq i\leq N}$，样本空间$\mathcal{X}\subset \R^d $，类别空间$\mathcal{Y}=\{1,2,...K\}$。最终需要求解后验分布$P(Y|X=x)$，同时将样本$x$划分到满足$\max _k P(Y=k|X=x)$的类别k中去。根据贝叶斯定理有
$$
\begin{align}
P(Y=k\;|\; X=x)&=\frac{P(Y=k,X=x)}{P(X=x)}\\
&=\frac{P(X=x\;|\;Y=k)P(Y=k)}{\sum_{k=1}^K{P(X=x\;|\;Y=k)P(Y=k)}}\\
&=\frac{p_k(x)\pi_k}{\sum_{j=1}^K{p_j(x)\pi_j}}
\end{align}\tag{23}
$$
其中$p_k(x)$代表类别k中关于x的概率分布；$\pi_k$为第k个类别的先验概率大小，同时满足$\sum_{k=1}^K{\pi_k}=1$. 这样就将所要求解的后验概率分布变换为已知的先验概率分布的函数的形式，上述等式实际上对几乎任何模型都成立。观察到，只要能够求解出$p_k(x)$就等价求解出了后验概率分布$P(Y=k|X=x)$.

关于如何求解$P(X=x|Y=k)$实际上就是如何规定或求解在每个类别中的样本是满足什么样的分布。关于这个问题，LDA给出了一个基本的模型假设：在任何一个类别中，假设样本分布都具有共同的协方差矩阵$\Sigma_k=\Sigma$；同时假设样本也满足高斯分布
$$
p_k(x)=\frac{1}{\sqrt{(2 \pi)^d} |\Sigma_k|^{\frac{1}{2}} }\exp{\left[-\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)\right]}\tag{24}
$$
其中$\Sigma_k=\Sigma $为类别k样本的协方差矩阵，满足$\Sigma_k=\sum_{x\in \mathcal{C_k}}{(x-\mu_k)(x-\mu_k)^T} $；$\mu_k$为第k类的均值向量，满足$\mu_k=\sum_{x\in \mathcal{C_k}}\frac{x}{|\mathcal{C_k}|} $. 

接下来利用式$(23)$对样本$x$进行类别分析，比较$x$在两个类别k和m之间的概率大小
$$
\ln{\frac{P(Y=k\;|\;X=x)}{P(Y=m\;|\;X=x)}}=\ln{\frac{p_k(x)}{p_m(x)}}+\ln{\frac{\pi_k}{\pi_m}}\\
=\ln{\frac{\pi_k}{\pi_m}}+x^T \Sigma^{-1} (\mu_k - \mu_m) - \frac{1}{2} (\mu_k + \mu_m )^T\Sigma^{-1} (\mu_k + \mu_m)\tag{25}
$$
使用对数比可以清晰地看到设施一个关于$x$的线性方程，结果是一个在$dim(x)=d$维空间的超平面。式$(25)$对任何两个类别都成立，因此两类类别空间的决策边界是线性的，任何两个类别的样本都能被决策边界，也就是d维超平面所分割开来。

从式$(25)$可以看到，LDA的分类函数实际上就是
$$
L_k(x)=x^T \Sigma^{-1} \mu_k -\frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k +\ln{\pi_k}\tag{26}
$$
最终要求解的模型为
$$
G(x)=\arg_{k}\max{L_k(x)}\tag{27}
$$
当然我们目前还不知道式$(26)$中的各个参数值，但是可以利用训练数据进行估计，下面给出这几个参数的估计式

* $\hat{\pi_k}=\frac{|\mathcal{C_k}|}{N}$，其中$|\mathcal{C_k}|$为标记为第k类的样本数，$N$为总样本数
* $\hat{\mu_k}=\sum_{x\in \mathcal{C_k}}\frac{x}{|\mathcal{C_k}|} $，表示第k类的均值向量
* $\hat{\Sigma_k}=\sum_{x\in \mathcal{C_k}}{(x-\mu_k)(x-\mu_k)^T} $，表示第k类的样本协方差

这是对于多分类问题最朴素的LDA算法，因为它是在原始的样本空间内，寻找一个或多个分类的决策超平面，但是事实上这会导致时间复杂度的增大。在实际应用过程中，我们不必这么做，应该如同二分类问题一样，也寻找一个低维超平面使得样本在这个超平面上更容易区分开来，但是这样一来模型就会变得复杂起来。

最后值得一提的是，LDA算法并没有要求训练数据一定满足高斯分布，数据可以是任意分布的，只是在实际应用过程中，需要考虑分类的效果，满足某些分布的数据可能会有很好的效果，而有些则不是；但是LDA的模型推导则包含了数据是高斯分布的假设，这是毫无疑问的。

#### *多分类LDA模型推导

多分类任务的假设如前面一节，假设数据集$D=\{(x_i,y_i)\}_{1 \leq i \leq N}$，同时样本空间为$\mathcal{X}\subset\R^d$，类别空间为$\mathcal{Y}=\{1,2,...K \}$. 这次我们需要将原始样本从d维空间投影到L维空间中，其中$L\leq K-1$被满足，那么分类超平面而不是直线应该为$W_{d\times L}$. 

LDA算法的思想并没有改变：使得原始样本被投影到低维超平面后，每个类别的中心均值向量能够相对于类内投影样本的协方差尽可能地分散，同样是保持最大化比值
$$
L(W)=\max {\frac{W^TM_bW}{W^TM_\omega W}}\tag{28}
$$
其中$M_b$为类间散度矩阵，$M_\omega$为类内散度矩阵。但是因为这是一个矩阵函数而不是标量函数，无法用二分类问题相同的解决方法。于是需要替代优化目标为
$$
W^*=\arg_{W}\max{\frac{\Pi_{diag}( w^T M_b w)}{\Pi_{diag}( w^T M_\omega w)}}\tag{29}
$$
其中$\Pi_{diag}​$是矩阵所有对角的元素乘积。这个优化目标实际上等价于求解多个w组合成W，并将其转化成可求解的问题
$$
\arg_{W}\max{\left\{ \frac{\Pi_{i=1}^L( w_i^T M_b w_i)}{\Pi_{i=1}^L( w_i^T M_\omega w_i)}
=\prod_{i=1}^L{ \frac{ w_i^T M_b w_i}{w_i^T M_\omega w_i} }
\right\}}\tag{30}
$$
式$(30)$转化成了广义瑞利商的形式，因此可用广义瑞利商的性质进行求解，根据广义瑞丽商的性质可以知道：最大的L个值就是矩阵$M_\omega ^{-1}M_b$的最大的L个特征值，那么最终的矩阵$W$就是最大的L个特征值所对应的特征向量。关于广义瑞利商的介绍在下面说明

##### 广义瑞利商

关于广义瑞利商，这里作简单介绍

给出瑞利商（Rayleigh quotient）的定义，有函数$R(A,x)$
$$
R(A,x)=\frac{x^HAx}{x^Hx}\tag{29}
$$
其中$A_{n\times n}$为n阶Hermitan矩阵，$x$为非零向量。Hermitan矩阵是满足共轭转置与自身相等的矩阵，即$A^H=A$，如果是实矩阵，则表示对称矩阵。

下面给出一个重要的引理：

**引理**

瑞利商$R(A,x)$具有这样的性质：函数的上界和下界分别是Harmitan矩阵的最大和最小特征值。也就是说
$$
\lambda_{min} \leq R(A,x)=\frac{x^HAx}{x^Hx}\leq \lambda_{max}\tag{30}
$$
其中$\lambda_{min}$为矩阵$A$最小的特征值；$\lambda_{max}$为矩阵$A$最大的特征值。特殊地有，若$x$为标准正交基，则$x^Hx=1$且瑞利商退化成$R(A,x)=x^HAx$.

定义：广义瑞利商（generalized Rayleigh quotient），有函数$R(A,B,x)$
$$
R(A,B,x)=\frac{x^HAx}{x^HBx}\tag{31}
$$
其中$x$为非零向量，$A_{n\times n},B_{n\times n}$为Hermitan矩阵，且$B$为正定矩阵。

**引理2**

广义瑞利商$R(A,B,x)$可以从形式上转化成瑞利商函数$R(A,B,x')$

对$x$进行变换
$$
x=B^{-\frac{1}{2}}x' \tag{32}
$$
分母部分可以变换为
$$
\begin{align}
x^HBx &=(B^{-\frac{1}{2}}x')^H B B^{-\frac{1}{2}} x'\\
&=x'^H	B^{-\frac{1}{2}} B B^{-\frac{1}{2}}x'\\
&=x'^Hx'
\end{align}\tag{33}
$$
变换过程中利用了$B$是正定矩阵的性质。然后分子部分变换为
$$
x^H A x=(B^{-\frac{1}{2}} x')^H A B^{-\frac{1}{2}} x'=x'^HB^{-\frac{1}{2}} A B^{-\frac{1}{2}} x'\tag{34}
$$
 最后将广义瑞利商变换成
$$
R(A,B,x')=\frac{x'^HB^{-\frac{1}{2}} A B^{-\frac{1}{2}} x'}{x'^Hx'}\tag{35}
$$
这实际上就是瑞利商$R(B^{-\frac{1}{2}} AB^{-\frac{1}{2}},x')$，那么可以根据引理得到
$$
\lambda_{min} \leq R(A,B,x')\leq\lambda_{max}\tag{36}
$$
其中$\lambda_{min},\lambda_{max}$分别为矩阵$B^{-\frac{1}{2}} AB^{-\frac{1}{2}}=B^{-1}A$的最小和最大特征值。

#### 线性判别分析的算法框架

下面给出广义线性判别分析的算法框架：

输入：训练数据集$D=\{(x_i,y_i)\}_{1 \leq i \leq N}$，同时样本空间为$\mathcal{X}\subset\R^d$，类别空间为$\mathcal{Y}=\{1,2,...K \}$.

输出：降维后的样本集$D′$，以及L维超平面$W$

1. 计算类内散度矩阵$ M_\omega $
2.  计算类间散度矩阵$ M_b $
3. 计算矩阵$M_\omega^{-1} M_b$
4. 计算矩阵$M_\omega^{-1}M_b$最大的L个特征值和L个特征值所对应的特征向量，合并特征向量得到投影矩阵$W$
5. 对样本$x$进行变换$z=W^Tx$
6. 得到降维后的样本$D'=\{z_i,y_i\}_{1\leq i\leq N}$

LDA算法不仅可以作为线性分类器，而且还可以对样本进行降维。在实际应用过程中，LDA更多地被应用在数据降维领域，和PCA共同成为比较常用的降维方法。关于PCA的方法会在之后的章节进行介绍。以上就是线性判别分析算法的基本流程。

### 实例【3.3】鸢尾花分类问题



求解鸢尾花分类问题，除了需要引入常用的几个库之外，我们还需要引入一下几个库：

```python
# 载入 python 自带鸢尾花分类问题数据集
from sklearn.datasets import load_iris as li
import sklearn.datasets as ds 
# 加载 python LDA 模块
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
# 加载 python QDA 模块
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
```





