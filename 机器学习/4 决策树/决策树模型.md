



10/14  21:49

下一步增加 图示，例题，实战



## 决策树模型

决策树是一种常用的回归和分类模型。以分类问题为例，决策树算法是一种基于树形结构的决策过程：基于样本的每一个特征取值对实例进行分类。做一个很简单的模拟，银行在评估外借风险时往往会按照贷款人的实际经济状况来决定是否借款，首先是否有房产可以抵押？如果没有的话，是否有担保人？如果没有担保人就不借，如果有担保人则担保人有房产可借，担保人没房产就不借。这样的决策过程就类似于一些if-else语句，同样这也是决策树的逻辑和结构。

![generaltree](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\generaltree.png)

<center>例子：上图为简化的借贷决策过程</center>

决策树的优点是：分类速度快。学习时，利用训练数据根据最优的属性划分建立决策树；预测时，利用预测的样本的所有特征进行判断，最终根据叶节点的分类进行划分类别。决策树通常有三个主要操作：选择最优属性划分，生成树，剪枝。现在的决策树模型主要是基于20世纪末提出的ID3，C4.5，CART算法。

### 决策树算法原理和框架

通常的决策树包含一个根节点，若干个内部节点和叶节点。根节点包含了所有样本集；内部节点对应了一个属性的测试条件；叶节点对应了样本的最终分类，即决策结果。从根节点到任意一个叶节点的通路代表了一个判断序列，根据从而根据这个判断序列能够决策出样本在该判断序列下的分类。决策树模型的最终目的是生成一个对未知样本的决策能力强，泛化能力强的决策树。

下面我们给出生成一颗决策树的基本流程：

约定 训练样本集 $D=\{(x_i,y_i)\}_{0\leq i\leq m}$特征属性集 $A=\{a_i\}_{0\leq i \leq m}$

```pseudocode
BuildTree(D,A)
begin
1 node= new treenode();
/* 情况1 */
2 if (D中样本类别c相同) then
3   node 被标记为c类的叶节点;return;
4 end if 
/* 情况2 */
5 if (A为空 or D在A上取值相同) then
6   node 被标记为c类的叶节点 其中c为D中类别众数;return;
7 end if 
/* 
按照不同的标准找到最优划分属性a
*/
8 计算最优划分特征 a;
9 for (a的一个取值ai) do
10  node 生成子节点 childnode;
11  Di表示D在属性a中取值为ai的样本集;
     /* 情况3 */
12  if (Di为空) then
13    childnode 被标记为c类的叶节点 其中c为D中类别众数;
14    return
15  else
16    childnode=BuildTree(Di,A-a)中treenode;
17  end if
18 end for
end
```

> 这一节讲解决策树原理 概率统计背景

我们可以看出整个决策树是递归结构建立的，通过反复找到最优划分属性$a$来标记当前节点所属的类别$c$.

这里说明一下三个递归终止条件：情况1：样本是同一种类$c$，就把当前节点标记为这个类$c$。情况2：没有属性可以选择 或 属性的取值对分类无影响，就标记为所有类别中出现频数最高的类$c$，表示无法划分。情况3：当前节点包含的属性$a$上取值为$a_i$样本集合为空，表示不能划分。



### 最优划分属性

最优划分属性是建立决策树的关键，同时也关乎决策树对未来数据分类预测的泛化能力的强弱。我们希望随着划分次数的增加，当前节点所含样本类别的纯度获得显著提升，从而达到对样本集的分类能力。

为了介绍决策树算法的核心：最优划分属性，我们需要引入两个概念：信息熵和信息增益。

#### 信息熵

为了直观地理解，不严谨地说，“熵”在物理学中的概念是一种描述微观量混乱程度的物理量，比如热力学中，两种气体分子的互相溶解导致了微观上的分子混乱程度的增大，因为此时同一片区域有两种气体分子在进行热运动。那么信息学上定义的信息熵和热力学熵在某种意义上具有相似性。信息熵表示的是对一个随机变量的不确定程度的度量，同时也可以看作用来度量某个集合的样本纯度，这两种理解方式在本质上是一致的。

我们先来看概率角度的定义：一个离散型随机变量 $X$ 的概率分布为
$$
P(X=x_i)=p_i \tag{1}\\
i=1,2,...N
$$
那么我们定义它的信息熵为
$$
Et(X)=-\sum_{i=1}^N{p_i log_2{p_i}} \tag{2}
$$
同时我们规定 $0log_20=0$ . 举一个很简单的例子，在二分模型（binary model）中 $X=\{0,1\}$，$X$ 的分布为
$$
P(X=1)=p  \ \ \ ,P(X=0)=1-p
$$
那么关于 $X$ 的熵可以表示为 
$$
Et(X,p)=-p\log_2{p}-(1-p)\log_2{(1-p)} \tag{3}
$$
还记得之前说过信息熵表示一个随机变量的不确定程度，那么在 $(3)$ 中，$p$ 的取值大小可以用来表示随机变量 $X$ 的不确定程度：若 $p=0,1$ 则 $X$ 被完全确定；若 $p=0.5$ 则 $X$ 的不确定度是最大的。这是来自我们直观的感受，不过用函数同样可以求出相同的结果：若 $p=0,1$ 则 $E(X,p)=0$；若 $p=0.5$ 则 $E(X,p)$ 取到最大值 1【图1】.也就是说，熵值越大，不确定程度越大。

那么在样本分类中信息熵又代表什么呢？对应于上面那个二分模型，随机变量 $X$ 的取值可以看作是一个样本集合中元素的取值，随机变量 $X$ 对每个取值的概率大小对应于每一类元素在样本集合中所占比例的大小。由熵值越大，随机变量的不确定程度越大的结论，我们可以推出在样本集合中，熵值越大，样本集合的分类越不明确；反之，一个样本集合的信息熵越小，得到的分类就越纯，越接近于我们想要的明确分类结果。

这里给出信息熵（information entropy）的第二个定义：信息熵是对样本集合纯度的度量。假设在当前样本集合 $D$ 中，第 $k$ 类样本所占的比例为 $p_k$ ，那么 $D$ 的信息熵
$$
Et(D)=-\sum_{k=1}^K{p_k\log_2{p_k}}\tag{4}
$$
其中 $k=1,2,...K$ 表示类别。同时我们还能看出 $(2)$ 与 $(4)$ 其实是等价的定义。同时还有关系式 $0\leq E(D)\leq log_2{K}$.

那么在决策树算法中，我们该按照什么样的标准来分割一个父节点为几个较小的子节点，同时又使得几个较小的子节点比之于父节点有更小的信息熵呢？我们需要量化地定义这个信息熵的差值。

#### 信息增益

信息增益（information gain）是用来度量集合 $D$ 以属性 $a$ 来划分后的信息熵的增量，即划分后纯度的提升程度的指标。对于划分标准属性 $a=\{a_i\}_{1\leq i\leq N}$ ，在 $a$ 上取值为 $a_i$ 的样本集合为 $D_i$ ，则样本集合 $D$ 以 $a$ 来划分后的信息增益为
$$
G(D,a)=Et(D)-\sum_{i=1}^{N}{\frac{|D_i|}{|D|}Et(D_i)} \tag{5}
$$

其中 $\frac{|D_i|}{|D|}$ 为取值为 $a_i$ 的样本数所占样本总数的比例。可以看到信息增益越大则划分后的样本纯度提升就越大，因此找到最优划分属性的过程，就是计算出 $A$ 中最大信息增益对应的属性 $a$ 的过程，就是  $a^*=arg_{a \in A}max{G(D,a)}$ .之后我们会提及，ID3 决策树就是用信息增益来选择最优划分属性的。

从概率角度来看，信息增益其实就是一个随机变量的确定程度提高程度的度量。

先把 $D,A$ 都看作随机变量，假设有联合概率分布 $P(D=D_k,A=A_j)=p_{k,j}$，再根据信息增益的定义
$$
G(D,A)=Et(D)-Et(D|A) \tag{6}
$$
其中
$$
Et(D|A)=\sum_{j=1}^{m}{P(A=a_j)Et(D|A=a_j)}\\
Et(D|A)=-\sum_{j=1}^{m}{P(A=a_j)\sum_{i=1}^K{\frac{|D_{i,j}|}{|D_j|}\log_2{\frac{|D_{i,j}|}{|D_j|}}}}  \\
Et(D|A)=-\sum_{j=1}^m{\frac{|D_j|}{|D|}\sum_{i=1}^K{\frac{|D_{i,j}|}{|D_j|}\log_2{\frac{|D_{i,j}|}{|D_j|}}}} \tag{7}
$$
这里规定了在样本集合 $D$ 中关于特征 $A$ 有 $m$  个不同的取值 $\{a_1,a_2,...a_m\}$ ，于是我们将样本按照特征 $A$ 的取值分成 $m$ 类 $\{D_1,D_2,...D_m\}$ ，同时在任意一个 $D_j$ 中的样本属于第 $i$ 类的样本子集合为 $D_{i,j}$ ，一共有 $K$ 个不同的类。其中$|\cdot|$ 定义为集合的大小。他们之间满足关系 $|D|=\sum_{j=1}^m{|D_j|}$ 以及 $|D_j|=\sum_{i=1}^K {|D_{i,j}|}$. $P(A=a_j)$ 实际上是在所有 $D$ 中样本中使得 $A=a_j$ 的概率，因为我们直接将样本 $D$ 按照 $A$ 的取值划分成了 $m$ 个子集，所以 $P(A=a_j)=\frac{|D_j|}{|D|}$. 一旦 $A=a_j$ 的值确定下来后，就可以把 $D_j$ 看作新的样本集合，求解 $Dj$ 的信息熵 $Et(D_|A=a_j)=Et(D_j)$，由此我们从概率上推导出和 $(5)$形式一致的式子。

> 这里列举一个简单计算实例

#### 信息增益比

但是事实上，信息增益存在一些问题，即对取值较多的特征具有偏向性。 于是我们试图将 $(5)$ 调整一下，定义信息增益比（information gain ratio）.

**定义：** 特征 $A$ 对样本集合 $D$ 的信息增益比 $G_r$ 定义为信息增益 $G(D,A)$ 与 $ D $ 关于 $A$ 的值的信息熵之比，即 
$$
G_r(D,A)=\frac{G(D,A)}{-\sum_{i=1}^m{\frac{|D_i|}{|D|}}\log_2{\frac{|D_i|}{|D|}}} \tag{8}
$$
其中 $A$ 的取值范围为 $\{a_1,a_2,...a_m\}$.

### 决策树算法

之前我们讲解了决策树算法的主要部分，即如何选择最优的特征进行样本分类。本节主要在之前的信息增益和信息增益比的原理上，讲解两个常用的决策树 ID3 和 C4.5 的生成算法，同时介绍为了提高泛化性能而做的剪枝处理，最后介绍一个经典的决策树 CART.

这里我们先给出一个具体的实例，根据一个人的属性来判断一个人是否具有偿还债务的能力。

**例1** 表1是一个由10个样本组成的债务偿还分类判断数据，数据包括了一个人的多个测量数据（特征）：第一个特征是房产拥有情况{是，否}；第二个特征是婚姻情况{单身，已婚，离婚}；第三个特征是年收入，最小值是60，最大值是220；最后一个特征是一个人的债务偿还能力

| ID   | 拥有房产（是/否） | 婚姻情况（单身，已婚，离婚） | 年收入（单位：千元） | 无法偿还债务（是/否） |
| ---- | ----------------- | ---------------------------- | -------------------- | --------------------- |
| 1    | 是                | 单身                         | 125                  | 否                    |
| 2    | 否                | 已婚                         | 100                  | 否                    |
| 3    | 否                | 单身                         | 70                   | 否                    |
| 4    | 是                | 已婚                         | 120                  | 否                    |
| 5    | 否                | 离婚                         | 95                   | 是                    |
| 6    | 否                | 已婚                         | 60                   | 否                    |
| 7    | 是                | 离婚                         | 220                  | 否                    |
| 8    | 否                | 单身                         | 85                   | 是                    |
| 9    | 否                | 已婚                         | 75                   | 否                    |
| 10   | 否                | 单身                         | 90                   | 是                    |

我们希望能够利用这些训练数据来学习一个决策树，用以未来对一个人的3个主要特征分析后能够间接得出他是否有能力偿还债务。

#### 连续值处理

在介绍ID3算法之前，我们需要进一步讨论如何利用最优划分属性进行样本的分类。比如有一个属性名为 "sex" 那么它的取值范围只能是 {"male","female"}，即我们能够很清楚地将当前结点中的样本下分为2个子集合，同时分为当前结点的2个孩子节点。但是如果有一个特征是 "height"或者，我们又该如何处理？首先 height 是一个连续值，不能明确地进行分类，那么我们就要先对 height 特征的值进行一定的预处理。

假设特征 a 是连续值特征，此时我们不能直接对特征 a 进行分类，我们首先需要将连续的属性离散化。关于离散化的方法特别多，其中最简单的就是二分法（bi-partition）处理。如果说属性 a 在样本中出现的值的集合为 $ \{a_1,a_2,...a_n\}​$, 如果我们找到一个点 $t​$ 将样本集合分成两类 $D_t^-​$ 和 $D_t^+​$. 其中 $D_t^-=\{d\in D|d(a)<t\}​$ 且 $D_t^+=\{d\in D|d(a)>=t\}​$，即样本中那些在特征 a 上取值小于 $t​$ 的组成一类，另一些在特征 a 上取值大于等于 $t​$ 的组成一类，若 $t=\frac{\min(a)+\max(a)}{2}​$， 这就是二分法。

但是如何保证划分点 $t$ 是最优的呢？可以这么理解，如果在这个连续属性和其他属性中我们要选一个信息增益最大的作为当前节点的划分属性，那么我们自然要选择当$t$使得这个连续属性的信息增益的最大值，来和其他属性的信息增益去比较，然后再取最大值作为当前节点的划分属性。我们用数学来表示：

将特征 a 进行升序排序后的的取值空间 $[a_1=\min(a),a_n=\max(a)]$ 分成 一段一段的小区间 $[a_i,a_{i+1})$ 的并，即使得在区间 $[a_i,a_{i+1})$ 中的任意点都具有相同的划分结果。于是，我们可以写出整个候选的划分点集合 $T(a)=\{\frac{a_i+a_{i+1}}{2}|1\leq i\leq n\}$ ，同时为了求出 $T(a)$ 中的最佳划分点，我们需要重新写信息增益
$$
G(D,a)=\max_{t\in T(a)}{G(D,a,t)}\tag{9}
$$

$$
G(D,a)=\max_{t\in T(a)}{Ent(D)-\frac{|D_t^-|}{|D|}Ent(D_t^-)-\frac{|D_t^+|}{|D|}Ent(D_t^+)}\tag{10}
$$

这里需要说明的是，在实际的求解过程中 a 和 t 都是变量，往往确定了 a 之后求解使得 $G(D,a)$ 最大的 t，最终求解出来的是一对变量 （a,t）.最终求解出来的 $t$  是使得在该连续值属性上能分割成纯度最大的两个样本子集的分割点. 二分法也是C4.5采用的机制。 

#### ID3算法与C4.5算法

ID3决策树是在每一个分类结点上应用信息增益指数来选择当前分类的最优特征，然后递归进行至整个决策树建立完成。整个过程是这样的：从根节点开始，对每一个特征 $a_i\in A$ 计算在根节点的信息增益 $G(D,a_i)$ ，并且选择信息增益的最大值所对应的特征 $a_k$ 作为当前结点的分类特征，然后根据在当前节点中的所有样本在特征 $a_k$ 上的不同取值划分成几个子集，然后将每一个子集作为新的结点，递归地进行下去。可以看出ID3算法实质上还是用极大似然估计进行概率模型的选择，即可以把最优的特征看作是“样本信息熵的梯度”。

> 例子计算

C4.5决策树算法流程和ID3决策树并没有什么很大的不同，只是在计算当前结点的最优划分属性的时候利用的是信息增益比指数。这两种决策树的算法框架在之前【】已经讲过，只不过区别是在具体求解当前结点的最优划分属性的时候分别采用了不同的方法。

> 例子计算



#### 树的剪枝

之前我们已经介绍了如何根据一个训练样本构造一棵分类决策树，那么接下来我们需要考虑：这棵树的泛化性能如何？需不需要对决策树进行后处理来提高它的泛化性能？假设我们根据样本学习构造的决策树对训练数据的拟合程度过高，即出现“过拟合”现象，那么我们就需要考虑它对未知样本的预测性能可能不会特别好，因为它把训练数据的一些极具特殊性的属性都学习过来而导致过拟合。因此在这种情形下，我们必须对树进行剪枝处理来提高泛化性能。

决策树剪枝常见策略主要是“预剪枝”与“后剪枝”。

“预剪枝”的基本思路为：在构造决策树的过程中，如果划分某个结点不能该决策树带来泛化性能的提升，就禁止它继续划分，并且把它标记为叶节点。“后剪枝”的基本思路为：在整棵决策树被构造完成后，需要从最底层的非叶子结点开始考虑，如果把当前结点的所有孩子都去除，且能给整个决策树带来泛化性能的提升，那么这种剪枝是有效的，同时把当前结点改成叶节点。如何计算树的泛化性能？我们会事先将训练数据集随机划分出一部分作为验证集，决策树对验证集分类的准确率作为树的泛化性能的一种参考度量。同时应该注意，训练集和验证集之间的比例应该合理。

> 图 剪枝案例

#### CART算法

CART树（classification and regression tree）的发明被誉为是数据挖掘史上的一个里程碑事件，广泛地被应用于数据挖掘领域。CART树在本质上是一个二叉树，也就是说，它是对每一个属性进行二分类操作得到的一棵决策树，它能同时应用于连续属性和离散属性的情形中。

在回归情形中，他主要利用最小二乘法的思想，利用损失函数最小化准则求解；在分类情形中，它以 Gini 指数最小化原则作为求最优划分属性来分类的标准。

假设有训练样本 $D=\{(x_i,y_i)\}_{1\leq i \leq n}$ 其中 $x$ 为输入变量， $y$ 为输出变量。

**回归情形**

加上假设输出变量 $y$ 是连续变量，即找到一棵决策树能够对 $x,y$ 进行回归预测。

我们最终会学习得到一个决策树模型：
$$
\hat{y}=f(x)\tag{11}
$$
同时样本空间会被我们划分成 $m$ 个区域，记为 $\{r_1,r_2,r_3,...r_m\}$ 这些区域是两两互不相交的，分对应于决策树的所有叶子节点：
$$
r_i\bigcap r_j=\phi\\
1\leq i,j\leq m \tag{12}
$$


假设在每个区域 $r$ 上都对应一个输出值 $c$ ，则我们可以更具体地将 $(11)$ 改写成：
$$
f(x)=\sum_{i=1}^m{c_iI(x\in r_i)} \tag{13}
$$
其中 $I(\cdot)$ 为指示函数，即若 $x\in r_i$ 条件成立，就输出 1；否则输出 0.

> 加图说明 路径上的 c

目前我们只需要知道一组 $\{c_i\}$ 的值以及空间的分布 $\{r_j\}$ 就能求出模型 $(12)$，那么该如何计算 $c$ 和 $r$ 呢？注意到，当 $r$ 的划分确定下来之后，我们其实就可以利用最小均方损失 $\sum_i{(y_i-f(x_i))^2}$ 求偏导得到所有 $c$ 的值。

求完偏导之后我们很容易看出其实 $c_i$ 的值就和区域 $r_i$ 中所有的样本有关：
$$
\hat{c_i}=E(y_j|x_j\in r_i) \tag{14}
$$
于是我们进一步地将问题归结为：如何求解最优的空间划分？答案是毋庸置疑的，就是利用误差最小化的原则。还记得我们说过，CART树其实是一个二分树，也就是说对一个区域在一次计算过程中被二分了，同时使得新的决策树具有更小的误差。那么最简单的一个思路其实就是找到样本点 $x$ 的某一个分量 $x^{(p)}$，在第 $p$ 维坐标轴上找一个最优分割点 $t$, 使得第 $p$ 分量小于等于 $t$ 的分为一类，大于 $t$ 的分为一类，然后不断地迭代这个过程，最终得到的就是最优的决策树。

具体地说就是找到 $p,t$, 对于某一个要划分的区域 $r$, 把它分成两个区域
$$
r(p,t)^-=\{x|x^{(p)}\leq t\}\\
r(p,t)^+=\{x|x^{(p)}>t\}\tag{15}
$$
其中最优的参数$p,t$是按照误差最小化的原则进行的：
$$
\min_{p,t}\{ \min_{c_-}{\sum_{x_i \in r(p,t)^-}{(y_i-c_-)^2}}+ \min_{c_+}{\sum_{x_i \in r(p,t)^+}{(y_i-c_+)^2}}\} \tag{16}
$$
对于求解 $(16)$ 中最优的 $p,t$ ，先遍历所有 $(p,t)$ 对的组合，然后在每个组合之下求解最优的 $c_-$ 和 $c_+$ .最终求解出来的 $c_-$ 和 $c_+$ 一定满足关系
$$
c_-=\frac{1}{N_-}\sum_{x_j\in r(p,t)^-}{y_j}\\
c_+=\frac{1}{N_+}\sum_{x_j\in r(p,t)^+}{y_j} \tag{17}
$$
这是满足约束条件
$$
\min_{c_-}{\sum_{x_i \in r(p,t)^-}{(y_i-c_-)^2}}+ \min_{c_+}{\sum_{x_i \in r(p,t)^+}{(y_i-c_+)^2}}
$$
的解.

然后对 $(16)$ 进行迭代地求解，最终能得到最优的回归CART决策树。

整理成算法框架就是

1. 根据当前的区域 $r$ 查找最优的参数 $p,t$

$$
\min_{p,t}\{ \min_{c_-}{\sum_{x_i \in r(p,t)^-}{(y_i-c_-)^2}}+ \min_{c_+}{\sum_{x_i \in r(p,t)^+}{(y_i-c_+)^2}}\}
$$

遍历分量 $p$, 然后遍历样本在第 $p$ 维上的取值作为 $t$，固定后求解出使得 $(16) $ 最小化的 $c_{-}$ 与 $c_{+}$ ，迭代后最终可求解出最优参数 $(p,t)$.

2. 作出由参数 $(p,t)$ 分割出的两个区域
   $$
   r(p,t)^-=\{x|x^{(p)}\leq t\}\\
   r(p,t)^+=\{x|x^{(p)}>t\}
   $$
   同时计算出区域所对应的输出值
   $$
   \hat{c_j}=\frac{1}{N_j}\sum_{x_i\in r_j(p,t)}{y_i}\\
   j=\{+,-\}\tag{18}
   $$



3. 继续重复 $2,3$ 的操作，直到建立的决策树满足误差条件
4. 生成由区域 $\{r_j\}$ 所对应的CART决策树，写出模型：

$$
f(x)=\sum_{i=1}^m{\hat{c_i}I(x\in r_i)}\tag{19}
$$

![regression](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\regression.png)

图为决策树的回归情形，可以看得出，当决策树的最大深度max_depth逐渐增大，对那些随机偏移较大的点也能拟合得很好，但是事实上，这存在过拟合的风险。因此我们需要验证集来选择合适的最大深度max_depth.

**分类情形**

对于分类情形，我们首先需要想到的就是如何重新定义损失函数 $(16)$ ，让它表示分类的错误率。在求解过程中，我们需要根据错误率的最小化准则来确定当前结点的最优化分属性和分割值 $t$. 同样地，保持CART树的思想不变，就是在分类过程中始终保持决策树是一棵二叉树。

这里介绍一个新的概念：Gini 指数。Gini 指数也是一个新的度量函数，它度量的仍然是样本集合的纯度：Gini 指数越小，所求解的样本集合纯度越高。Gini 指数的减小和错误率的减小是一致的，因为当前样本纯度的提高，就意味着被认为是样本中所占比例最大的那个类的这个决策的可靠性越高。

对于一个 $K$ 分类问题，样本的 Gini 指数的定义为
$$
Gini(D)=1-\sum_{j=1}^K{(\frac{|D_j|}{|D|})^2} \tag{20}
$$
其中 $\frac{|D_j|}{|D|}$ 表示第 $j$ 类样本所占的比例大小。

假设当前结点所对应的样本集合为 $D$, 需要找到一个划分特征 $A$  和特征下的一个分割值 $a$ 来将样本二分成两个子集合，同时保持 Gini 指数的最小化。
$$
Gini(D,A,a)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2) \tag{21}\\
$$
其中
$$
D_1=\{(x,y)|x(A)=a\}\\
D_2=\{(x,y)|x(A)\neq a\}
$$
具体实现CART分类树的算法框架为

从根节点开始进行递归操作

1. 当前结点包含样本 $D$ ，遍历选择特征 $A$ 与特征 $A$ 下的取值 $a$ ,  对每一组值 $(D,A,a)$ 进行计算 Gini 指数的值

$$
Gini(D,A,a)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

2. 选择使得 Gini 指数最小的一组值 $(A,a)$ 对样本 $D$ 进行分类，生成样本子集 $D_1$ 和 $D_2$ ，同时在当前结点下增加两个子结点
3. 对每一个子结点判断是否满足递归终止条件：样本为同一类？是否还包含可分类的特征？分类的特征是否是单一取值？如果满足递归终止条件就标记为包含比例最多的类的叶结点，同时终止递归；如果不满足，就对样本子集继续递归
4. 生成 CART 决策树



![compare_entropy](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\compare_entropy.png)

![compare_gini](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\compare_gini.png)

图？两个图是两个高斯分布样本，运用了两种不同的度量方法：Gini指数和信息熵得到的分类情况。其中蓝色的点（正样本）满足正态分布$N(\mu_1=1,\mu_2=1,\sigma_1^2=1,\sigma_2^2=1,\rho=0)$；黄色的点（负样本）满足正态分布$N(\mu_1=1,\mu_2=1,\sigma_1^2=1,\sigma_2^2=1,\rho=0)$.  红色的区域代表该区域中的所有点都被分类为正样本；蓝色的区域代表该区域中的所有点都被分类为负样本；黄色的区域表示在该区域中的样本分类不确定。首先，我们可以注意到，决策树的边界是由多条直线分割形成的，这也对应了决策树的特点，决策树是根据一个属性的取值的不同划分成相应不同的类别，例如，此处的属性就是$X,Y$，因此对某个属性的不同取值进行划分类别也就是相当于在沿着平行于$X$轴或$Y$轴的方向切一刀了，整体来看，每个从根节点到叶子结点的路径就是一个区域被分割形成的过程，而这个区域中的样本点是同一个类别的。其次，我们可以看到运用两个不同的方法进行分类，最后结果形成决策边界是不同的，第一个图运用的是信息熵，第二个图运用的是Gini指数，可以注意到，在样本分布 比较密集的区域，两者的决策边界的区别不是很大，而在样本分布比较稀疏的区域，两者的决策边界有很大的区别，这说明在样本数越多的情形下不同指数的计算结果是几乎一致的，在样本比较少的情形下相差较大，不确定程度越高。

![max_depth=1](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\max_depth=1.png)

![max_depth=4](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\max_depth=4.png)

图？为决策树的分类情形中max_depth=1和4的情形，可以看出，max_depth相当于是分割直线的数量。两个max_depth都不能很好地拟合数据，因为都存在明显的误分，因此我们要适当地增大max_depth.

#### CART树的剪枝

生成一棵决策树，如果想要决策树具有非常好的泛化性能而不因为数据过拟合导致预测的不准确，我们就必须对其进行剪枝处理。CART 树同样是如此，我们通常的处理方式是从树的底端开始逐次剪去一些导致数据过拟合的结点。也就是说，如果一棵树的枝叶越多，树的叶节点数目越大，就越有可能会导致其发生过拟合现象。基于这个观察，我们可以定义一个损失函数
$$
Cost_\alpha(T)=Cost(T)+\alpha |T| \tag{22}
$$
其中 $T$ 为以 $t$ 为根结点的子树，$|T|$ 为以 $t$ 为根结点的树的叶节点个数，$Cost(T)$ 是训练集在树 $T$ 上的误差度量，可以是Gini 指数或者是$(16)$ 式的损失函数等等。约束 $\alpha\geq0$ , 是未知参数，$\alpha|T|$ 实际上是对树叶结点数量的惩罚项，很显然，当 $\alpha \rightarrow +\infty$ 时，$T$ 只包含一个根节点；当 $\alpha=0$ 时，不进行剪枝的树是最优的，但是肯定会出现过拟合现象；换句话说，参数 $\alpha$ 越小，则最终求得的决策树越大；反之参数 $\alpha$ 越大，最终建立的决策树就越小。

因此我们对CART树的剪枝算法主要是基于这样一种思想：

因为我们事先并不知道参数 $\alpha$取怎样一个合适的值使得决策树的拟合和预测能力最佳，因此我们不妨将参数$\alpha$的取值分为许多小区间段 $\cup [\alpha_i,\alpha_{i+1})$ ，其中 $\alpha_i$满足
$$
\alpha_0<\alpha_1<...\alpha_i<\alpha_{i+1}<...\alpha_n<\infty\\
i=1,2,...n
$$
递归地利用参数 $\alpha\in [\alpha_i,\alpha_{i+1})$剪枝得到树为$T_i$，那么对每一个区间中的参数$\alpha$ 进行剪枝得到的一个最优子树序列为$\{T_i\}_{1\leq i\leq n}$. 因为我们是利用递归方法剪枝，所以最后剪枝得到的最优子树序列是逐层嵌套的 $T_i\sub T_{i+1}$,$\forall 1\leq i\leq n$.

我们先对一个具体的决策树的内结点开始讨论，假设当前结点为 $t$ ，对于以 $t$ 为根的一棵子树来说，损失函数为
$$
Cost_\alpha(T_t)=Cost(T_t)+\alpha |T_t| \tag{23}
$$
如果结点 $t$ 是单结点的，没有子孩子，那么可以改写为
$$
Cost_{\alpha}(t)=Cost(t)+\alpha \tag{24}
$$
我们的目标是找到一个合适的$\alpha$值，使得 $Cost_{\alpha}(t)=Cost_{\alpha}(T_t)$ . 这样就满足了我们的剪枝条件，也就是说，对于当前给定的 $\alpha$ 值，树 $T_t$ 和单结点 $t$ 的损失函数值是相同的，即两者在能力上是等价的.
$$
\alpha=\frac{Cost(t)-Cost(T_t)}{|T_t|-1} \tag{25}
$$
那么这个时候，两棵树的分类能力是相同的，我们当然是取比较简单的一棵，即把多余的枝叶减去，只留下根结点 $t$ . 

然后我们继续增加参数$\alpha$的值，对之前剪枝后的树继续剪枝，一致进行到根节点为止，此时我们得到了最优子树序列$\{T_i\}_{1\leq i\leq n}$.

最后我们用事先准备的验证数据集对最优子树序列中的每一个子树$T_i$计算验证集在该树上的损失，一般用均方损失函数或者是基尼指数。损失最小的决策树是最优的决策树，那么最终我们的最优CART树就被确定了，同时参数 $\alpha$的值也被确定了.

以上过程整理成算法框架就是CART决策树的剪枝算法：

1. 初始化参数 k=0, $T=T_0$, $\alpha=+\infty$
2. 从底向上对每个当前树的内部结点计算 $Cost(T_t)$, $|T_t|$

$$
\beta(t)=\frac{Cost(t)-Cost(T_t)}{|T_t|-1}\\
\alpha=\min(\beta(t),\alpha)
$$

3. 对满足$\beta(t)=\alpha$的那个内部结点进行剪枝处理，同时把该结点标记为类别占比最大的那个类，把当前树标记为 $T$.
4. 更新参数 k=k+1,$\alpha_k=\alpha$, $T_k=T$
5. 判断：如果$T_k$是根节点树或者是两个叶结点构成的树，则$T_k=T_n$并且递归终止；否则继续递归，回到2
6. 终止递归后，交叉验证法在最优子树序列中找到最优决策树.



### 实例：红酒品种鉴定

红酒有各种各样的品种，那么如何按照红酒的典型特征对他们进行鉴定和分类呢？这实质上就是一个典型的分类学习问题。这一节我们运用Python中的决策树对红酒进行分类和鉴定。我们先给出数据集的来源 http://archive.ics.uci.edu/ml/datasets/Wine.

数据集包括两个文件，刚下载下来都是.name格式，需要我们自行转换成csv格式。第一个文件名为winenames，其中包括了对数据集的描述；第二个文件名为winedata，其中包括了数据集的所有特征值和分类结果。

#### 数据处理

刚拿到的数据，我们首先需要查询数据的大小、格式、是否完整、特征信息等，从而启发我们给出正确且合适的模型来求解。

我们首先要做的自然是输出数据集的描述，因为在描述文本中，几乎包括了数据集的所有信息.

``` python
# 打开数据描述文件
f=open('C:\Users\lenovo\Desktop\wine_names.csv')
names=f.read(10000)
# 打开数据内容文件
data=pd.read_csv('C:\Users\lenovo\Desktop\wine_data.csv')
print names,data
data=data.values
Num=data.shape[0] #获取数据的数量
FeatureNum=data.shape[1]-1 #获取数据的特征数
# 将数据分成 data & target 两部分
Y=data[:,0]
X=data[:,1:FeatureNum+1]
```

上面只是简单的对数据内容和数据描述进行打印，同时将数据分成特征和目标两部分，方便进行学习. 这里给出数据的主要描述.

数据集主要包括了13个特征：

| Alcohol                                 | 乙醇含量         |
| :-------------------------------------- | ---------------- |
| Malic acid                              | 羟基丁二酸含量   |
| Ash                                     | 灰分含量         |
| Alcalinity of ash                       | 灰分碱度含量     |
| Magnesium                               | 镁含量           |
| Total phenols                           | 总酚含量         |
| Flavanoids                              | 黄酮素类含量     |
| Nonflavanoid phenols                    | 非黄烷烃酚类含量 |
| Proanthocyanins                         | 原花色甙含量     |
| Color intensity                         | 彩色亮度         |
| Hue                                     | 色调             |
| OD280/OD315 of diluted winesD 280/D 315 | 稀释葡萄酒含量   |
| Proline                                 | 脯氨酸含量       |

同时给出了关于数据格式的描述：

| **数据集特性:** | 多变量     | **实例个数:** | 178  | 领域:**           | 物质类     |
| --------------- | ---------- | ------------- | ---- | ----------------- | ---------- |
| **属性特性:**   | 整数，实数 | **属性个数:** | 13   | **捐献时间**      | 1991-07-01 |
| **任务:**       | 分类       | **缺失值?**   | No   | **网页点击次数:** | 984473     |

所有的属性都是连续型的；第一列是他们的分类目标1-3；没有包含缺失值；总共有13个特征.

由于所有特征值都是连续数值型的，因此我们不需要做过多的数据预处理

了解了数据的基本格式后，我们需要知道一下数据的分布情况，有时数据的分布情况能体现不同特征之间的相关性，启发我们使用什么模型来求解.

![wine_distribution](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\wine_distribution.png)

图？ 将数据集的13个特征两两组合的二维平面投影分布情况，在刚接触到数据的时候，我们应该想到的是尽可能使用简单的模型来求解，比如线性模型，此时就要分析每两个特征之间的线性相关性，如果数据分布能够大致看得出线性相关性的话，一般我们会先用线性模型来尝试，这里某些子图上看得出数据散点之间呈很好的线性相关性：明显的线状分布且三个类别中心之间分得较开，很容易分割；但是某些子图数据散点的线性相关性明显差些. 为了篇幅有限，我们就直接用决策树模型来分类.

#### 模型训练

首先，我们要对数据集进行分割：分割成训练数据和验证数据. 之前已经在sklearn.model_selection包中讨论过了train_test_split函数.

```python
from sklearn.model_selection import train_test_split
[X,Xtest,Y,Ytest]=train_test_split(X,Y,test_size=0.3,shuffle=True)
```

接着，分割好了数据且数据完好，可以正式开始训练了，有必要先介绍一下决策树的分类器函数，它带有很多参数，这里简单说一下它的用法：一般使用默认参数即可，如果对模型的最终结果具有较高的要求可以改动参数，常用的参数改动包括criterion, max_depth, min_impurity_split, min_samples_leaf 等等.

```python
from sklearn.tree import DecisionTreeClassifier as DTC
DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)
```

DecisionTreeClassifier的参数

- **criterion** : string, optional (default=”gini”)

  用来度量分割的质量. 支持的有"gini"gini不纯度和"entropy"信息增益.

- **splitter** : string, optional (default=”best”)

  用于在每个节点选择拆分的策略。支持的策略是“best”选择最佳分割，“random”选择最佳随机分割。

- **max_depth** : int or None, optional (default=None)

  树的最大深度。如果没有，那么节点会被展开，直到所有的叶子都是纯的，或者直到所有的叶子包含的样本少于min_samples_split .

- **min_samples_split** : int, float, optional (default=2)

  拆分内部节点所需的最小样本数：如果是int类型，则将min_samples_split作为最小数目。如果float型，则min_samples_split 是一个分数，而ceil(min_samples_split * n_samples) 是每个分块的最小样本数。

- **min_samples_leaf** : int, float, optional (default=1)

  一个叶节点所需的最小样本数。任何深度的分裂点只有在每条左右枝上至少留下至少min_samples_leaf训练样本时才会被考虑，这可能会使模型平滑，特别是在回归中，如果是int，则以最小样本_叶作为最小数目。如果是浮点数，则min_samples_leaf是一个分数，而ceil(min_samples_leaf * n_samples)是每个节点的最小样本数。

- **min_weight_fraction_leaf** : float, optional (default=0.)

  (所有输入样本的)权重之和的最小加权分数，需要在一个叶节点上。当不提供样品重量时，样品具有相同的重量。

- **max_features** : int, float, string or None, optional (default=None)

  在寻找最佳拆分时要考虑的特征数量：如果为int，则考虑每次拆分时的max_feature。如果是浮点数，则max_features是一个分数，int(max_features * n_features)特征在每个拆分处都被考虑。如果是“auto”，则max_features=sqrt(n_features)。如果是“sqrt”，则max_features=sqrt(n_features)。如果是“log2”，则max_features=log2(n_features)。

- **random_state** : int, RandomState instance or None, optional (default=None)

  如果int，random_state 是随机数生成器使用的种子；如果是RandomState实例，andom_state是随机数生成器；如果没有，则随机数生成器是np.random使用的RandomState实例。

- **max_leaf_nodes** : int or None, optional (default=None)

  以best-first的方式建立一棵具有max_leaf_nodes的树。最佳节点被定义为不纯度的相对减少。如果None，那么叶节的数量是无限的。

- **min_impurity_decrease** : float, optional (default=0.)

  如果这种拆分导致大于或等于此值的不纯度减少，则节点将被拆分。加权不纯度减少方程如下N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) 其中N是样本总数，Nt是当前节点的样本数，Nt_L是左子节点中的样本数，Nt_R是正确子节点中的样本数。N，N_t，N_t_R和N_t_L都是指加权和，如果样本权重被传递进去的话。

- **min_impurity_split** : float

  树扩展时早期停止的阈值。如果一个节点的不纯度超过阈值，它就会分裂，否则它就是叶子结点。

- **class_weight** : dict, list of dicts, “balanced” or None, default=None

  类的权重表达形式 {class_label: weight}. 如果不给定，所有的类别都应该有一个权重。对于多输出问题，dict列表能够以与y的列相同的顺序给定。注意，对于多输出(包括多标签)，应该为每个列的每个类定义其自己的dict中的权重。

- **presort** : bool, optional (default=False)

  是否预排序数据以加快拟合中最佳分裂的查找。对于大型数据集上决策树的默认设置，将其设置为true可能会减慢学习过程。当使用较小的数据集或有限的深度时，这可能会加快学习的速度。

DecisionTreeClassifier的属性

- **classes_** : array of shape = [n_classes] or a list of such arrays

  分类标签(单输出问题)，或一个数组的类标签(多输出问题)。

- **feature_importances_** : array of shape = [n_features]

  返回特征的重要性。

- **max_features_** : int,

  max_features的推断值。

- **n_classes_** : int or list

  类的数量(对于单个输出问题)，或者包含每个输出的类数的列表(对于多输出问题)。

- **n_features_** : int

  执行fit时的特征数。

- **n_outputs_** : int

  执行fit时的输出数。

- **tree_** : Tree object

  基础树对象.。

```python
# 利用method criterion来完成决策树的分类
def fit_wine_data(method):
    # 定义一个DecisionTree分类器 以method方式
    dtc=DTC(criterion=method)
    # 对数据进行拟合
    model=dtc.fit(X,Y)
    # 计算model拟合的训练评分
    train_score=model.score(X,Y)
    # 计算model验证评分
    test_score=model.score(Xtest,Ytest)
    print('train score %.8f \ntest score %.8f\n' %(train_score ,test_score))  # 格式化输出结果
    return [train_score,test_score]

# 用entropy方式进行训练
[trains,tests]=fit_wine_data('entropy')
```

给出正确输出结果

```python
train score 1.00000000 
test score 0.92592593
```

从输出结果来看，训练的结果非常完美，但是验证的结果就有点偏低，这是由于模型的过拟合导致的。因为对训练数据的过度拟合，导致了我们构造的决策树的泛化能力不高。目前解决的方法就是决策树的剪枝，通过限制DecisionTreeClassifier的参数来限制树的结构，从而实行剪枝。之后在模型的优化部分会讨论。

![winetree](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\winetree.png)

图？为决策树的最终结构，所有叶子节点都代表了一个类别，每个节点都包含了samples样本数，entropy信息熵，values代表每个类别的样本数。

在实际应用过程中，我们需要画出不同类别之间的边界，这样能从图中更加清晰和直观地看出分类结果，下面我们用python的matplotlib库来实现这个结果。

```python
# plot contour surface
def plot_contour_surface(X,Y,FeatureNum):
    # 定义figure 大小30*30
    Fig=plt.figure(figsize=(30,30))
    # 对特征两两组合
    for i in range(FeatureNum):
        for j in range(FeatureNum):
            # 作出子图
            fig=plt.subplot(FeatureNum,FeatureNum,i*FeatureNum+j+1)
            if i!=j:
                # 对特征模型拟合
                dtc=DTC(criterion='gini')
                submodel=dtc.fit(np.c_[X[:,i],X[:,j]],Y)
                # 形成平面内的网格点
                num=100 # 数量
                [minx1,maxx1]=[min(X[:,i]),max(X[:,i])]
                [minx2,maxx2]=[min(X[:,j]),max(X[:,j])]
                [xx,yy]=np.meshgrid(np.linspace(minx1,maxx1,num),
                                    np.linspace(minx2,maxx2,num))
                # layout 调整
                plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
                # 对网格点预测结果
                Z = submodel.predict(np.c_[xx.ravel(), yy.ravel()])
                Z = Z.reshape(xx.shape) # 调整矩阵形状
                # 绘制等高线
                cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)
                # 绘制数据散点
                fig.scatter(X[:,i],X[:,j],s=15,c=Y,cmap=plt.cm.RdYlBu, edgecolor='black')
                fig.set_xlim(min(X[:,i]),max(X[:,i]))
                fig.set_ylim(min(X[:,j]),max(X[:,j]))
            else:
                label='Feature'+str(i+1)
                # 添加label
                fig.text(0.5, 0.5,label, fontsize=25,
                horizontalalignment='center',
                verticalalignment='center') # 居中
    plt.show()
    return
```

上面的代码经过了这样的过程：先定义了一个figure，其中共有13*13个子图，在每个子图中，再对13个特征每两个组成一个pair，对这两个特征进行拟合，随后在二维平面内均匀生成一定数量的网格点，在网格点的基础上进行预测和分类，最后绘制等高线和加入散点与label就完成了绘制工作。有必要介绍一下numpy.c_[]是将两个或多个数组进行拼接，array.ravel()是对多维数组进行降至一维处理。

![wine_map](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\wine_map.png)

图?为上面代码生成的以决策树为模型的红酒数据二维决策边界图。其中红、黄、蓝分别代表三个类别，数据散点的颜色和所属类别区域的背景颜色相对应。

#### 模型优化

这一部分主要做一些模型的参数优化，如何找到合适的参数值使得DecisionTreeClassifier具有更强的分类能力和泛化性能。我们一般优化一个参数的时候往往使用穷举和搜索，最终可以得到关于参数值和评分的函数曲线，从图片中可以很清晰很直观地找到最优参数的值。

我们先对一个参数进行优化，在一个范围内改变max_depth的值，绘制出训练评分和验证集的评分曲线，找到最优的参数值。

```python
# para1=max_depth 对数据进行拟合
def fit_wine_data_depth(method,para1):
    # 数据拟合
    dtc=DTC(criterion=method,max_depth=para1)
    model=dtc.fit(X,Y)
    # 模型训练评分
    train_score=model.score(X,Y)
    # 模型拟合评分
    test_score=model.score(Xtest,Ytest)
    print('train score %.8f \n test score %.8f\n' %(train_score ,test_score))  # 格式化输出
    return [train_score,test_score]
```

这是对当para1==max_depth的时候模型对数据的一次拟合，同时fit_wine_data_depth会返回训练评分和拟合评分。

```python
# 寻找最优max_depth并绘制曲线
def plot_wine_data_depth():
    N=100 # 点数量
    depths=np.linspace(2,200,N)
    fig=plt.figure(figsize=(10,10))
    trainsc=np.zeros(N)
    testsc=np.zeros(N)
    
    # 对depths中所有参数进行拟合
    for i in range(N):
        # 返回评分结果
        x=fit_wine_data_depth('entropy',depths[i])
        trainsc[i]=x[0]
        testsc[i]=x[1]
        
    # 绘制评分曲线
    plt.plot(depths,trainsc,'.r--',label='train score')
    plt.plot(depths,testsc,'g',label='test score')
    plt.scatter(depths,testsc,c='y')
    # 图像基本设置
    plt.grid()
    plt.title('train score & test score related with max_depth')
    plt.xlabel('max_depth')
    plt.ylabel('score %')
    plt.legend()
    plt.show()
    
    # 计算最优参数
    r=np.max(testsc)
    idx=np.where(testsc==r)
    print r,idx
    return [r,idx]
```

这里在循环里调用了fit_wine_data_depth函数，同时返回最优的结果，并绘制出误差曲线。

![遍历max_depth](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\遍历max_depth.png)

图？为上面的代码运行结果，绿色为模型的验证评分曲线，红色的是模型训练评分曲线，横轴表示max_depth也就是树的最大深度，纵轴表示模型对数据的评分。

这段程序的最终运行结果在这里给出

```python
0.981481481481 (array([41, 51, 89], dtype=int64),)
```

其中第一个代表了模型的最高验证评分，第二个表示数组depths在索引41，51和89的位置是最优参数的值。

那么下一个要优化的参数就是min_impurity_split最小分割的信息熵值，具体意义参照DecisionTreeClassifier的参数介绍。注意，最新的sklearn已经在官方公告中说明，之前的min_impurity_split之后将被更新成min_impurity_decrease，同样地

```python
# para1=min_impurity_decrease 数据拟合
def fit_wine_data_impurity(method,para1):
    # 数据拟合
    dtc=DTC(criterion=method,min_impurity_decrease=para1)
    model=dtc.fit(X,Y)
    # 模型训练评分
    train_score=model.score(X,Y)
    # 模型拟合评分
    test_score=model.score(Xtest,Ytest)
    return [train_score,test_score]
```

然后再利用枚举的手段，利用plot_wine_data_impurity调用函数fit_wine_data_impurity.

```python
# 寻找最优min_impurity_decrease 并绘制曲线
def plot_wine_data_impurity():
    N=100 # 点数量
    impurity=np.linspace(0,0.2,N)
    fig2=plt.figure(figsize=(10,10))
    trainsc2=np.zeros(N)
    testsc2=np.zeros(N)
    for i in range(N):
        x=fit_wine_data_impurity('gini',impurity[i])
        trainsc2[i]=x[0]
        testsc2[i]=x[1]
    plt.plot(impurity,trainsc2,'.r--',label='train score')
    plt.plot(impurity,testsc2,'g',label='test score')
    plt.scatter(impurity,testsc2,c='y')
    plt.title('train score & test score related with max_depth')
    plt.grid()
    plt.xlabel('min impurity')
    plt.ylabel('score %')
    plt.legend()
    plt.show()
    r=np.max(testsc2)
    idx=np.where(testsc2==r)
    print r,idx
    return [r,idx]
```

最终得到的结果是

```python
0.981481481481 (array([3, 6], dtype=int64),)
```

显然调整这个参数对最后的验证评分影响和max_depth同样明显，得到的曲线如下

![遍历min_impurity](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\遍历min_impurity.png)

对于如何同时优化模型的多个参数，我们目前还没有很高效的解决方案，但是sklearn.model_selection给我们封装了非常高效的GridSearchCV. 下面简单介绍一下这个优化器.

GridSearchCV(*estimator*, *param_grid*, *scoring=None*, *fit_params=None*, *n_jobs=None*, *iid=’warn’*, *refit=True*, *cv=’warn’*, *verbose=0*, *pre_dispatch=‘2\*n_jobs’*, *error_score=’raise-deprecating’*, *return_train_score=’warn’*)

**GridSearchCV的常用参数**

- **estimator** : estimator object.

  假设这是为了实现scikit-learn estimator 接口。估计器需要提供score函数，或者必须传递scoring。

- **param_grid** : dict or list of dictionaries

  以参数名称(字符串)作为键和参数设置列表作为值的字典，或者这类字典的列表，在这种情况下，将搜索列表中每个字典所生成的网格。这允许对任何参数设置序列进行搜索。

- **scoring** : string, callable, list/tuple, dict or None, default: None

  一个单独的string或一个可调用对象来评估测试集上的预测。为了评估多个指标，要么给出(唯一的)字符串列表，要么给出一个名为键，可调用为值的字典。注意，当使用自定义scoring时，每个scorer应该返回一个值。返回一个列表/值数组的metric函数可以包装成多个评分器，每个评分器返回一个值。请参见为一个示例指定多个评估指标。如果没有，则使用估计器的默认记分器(如果可用)。

- **fit_params** : dict, optional

  参数传递给fit方法。

- **cv** : int, cross-validation generator or an iterable, optional

  确定交叉验证拆分策略。cv的可能输入是：None，使用默认的3倍交叉验证整数，指定(分层)kFold中的Folds，该对象用作交叉验证生成器。一个iterable对象服从train_test_split. 对于整数/无输入，如果estimator是分类器，并且y是二类或多类，则使用StratifiedKFold。在所有其他情况下，都使用KFold。参考用户指南的各种交叉验证策略，可在这里使用。更改版本0.20：CV默认值，如果没有将从3倍到5倍，在v0.22.

- **verbose** : integer

  控制细节：越高，信息越多。


**GridSearchCV的常用属性**

* **cv_results_** : dict of numpy (masked) ndarrays

  以键作为列名，值作为列，可以导入到pandas.DataFrame中的字典。其中常用的主键为

  ```python
  {
  'mean_test_score','std_test_score','mean_train_score','std_train_score','mean_fit_time',
  'std_fit_time','mean_score_time'  ,'std_score_time'     
  }
  ```

* **best_estimator_** : estimator or dict

  由搜索选择的estimator，即给出的数据上的最高分数(或最小损失)的estimator。如果refiit=false，则不可用。

* **best_score_** : float

  最佳估计器的平均交叉验证分数，用于多指标评估时只有在指定了refit时才会出现。

对于单个参数的搜索和优化，给出如下的例子，能让读者体会到GridSearchCV的用法

```python
# 利用 GridSearchCV 对单个参数进行优化
def GridSearchSinglePara():
    N=50 # 点数量
    max_depth=np.linspace(1,200,N)
    hyperpara={'max_depth':max_depth} # 参数字典
    # GridSearchCV 对象
    model=GSCV(DTC(),hyperpara,cv=5)
    model.fit(X,Y)
    print model.best_params_,model.best_score_
    # 之后作图
    
```

一开始先定义点的数量，然后令max_depth为1-200的等间隔一定数量的点组成，hyperpara是一个字典，其中以需要优化的参数作为key，参数值的搜索范围作为value，可以定义多个。对于GSCV对象，以DecisionTreeClassifier作为estimator，hyperpara作为要优化的参数字典，cv=3-5的整数，返回一个对象model。最后输出model的两个属性：最佳参数和最佳评分。

```python
# 作图部分
R=model.cv_results_
# 平均训练评分
mtrains=R['mean_train_score']
# 标准训练评分
strains=R['std_train_score']
# 平均验证评分
mtests=R['mean_test_score']
# 标准验证评分
stests=R['std_test_score']
# 作图
fig=plt.figure(figsize=(10,10))
# 填充
plt.fill_between(max_depth,mtrains-strains,mtrains+strains,color='lightgray',alpha=0.3)
plt.fill_between(max_depth,mtests-stests,mtests+stests,color='lightgray',alpha=0.3)
# 曲线
plt.plot(max_depth,mtrains,color='r',label='train mean scores')
plt.plot(max_depth,mtests,color='g',label='test mean scores')
# 图的基本设置
plt.grid()
plt.legend()
plt.title('max_depth gridsearch')
plt.xlabel('max_depth')
plt.ylabel('score %')
plt.show()
```

GridSearchCV.cv_results_包括了平均或标准训练评分、验证评分等信息，方便我们作出参数优化过程中每个hyperpara列表中参数所对应的训练评分，最终得到直观的反馈。

![meanstdscore_depth](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\决策树\meanstdscore_depth.png)

知道了GridSearchCV的基本用法和性质，最后介绍GridSearchCV最重要的应用，实现多参数优化，需要优化的参数有'criterion'， 'min_impurity_decrease'，'max_depth'其中包括了离散值参数和连续值参数。

```python
# 利用 GridSearchCV 同时优化多个参数
def GridSearchPara():
    N=200
    impurity=np.linspace(0,0.5,N)
    hyperpara={'criterion':['gini','entropy'],
               'min_impurity_decrease':impurity,
               'max_depth':np.linspace(1,200,N)}
    model=GSCV(DTC(),hyperpara,cv=5)
    model.fit(X,Y)
    print model.best_params_,model.best_score_
    return
```

可以得到最后的模型最优求解结果为

```python
{'min_impurity_decrease': 0.034343434343434343, 'criterion': 'entropy', 'max_depth': 3.0101010101010099} 0.975609756098
```

和原本的结果相比

```python
train score 1.00000000 
test score 0.88888889
```

可以看到很明显的提升，因为我们当时的数据是按照原来的样本随机切分成训练数据和验证数据的，所以结果会有一定的随机性，但是这不妨碍参数的优化，结果的提升非常明显。

### 总结

本章节主要介绍了决策树的种类，基本思想和原理，数学推导，sklearn中的使用方法等。

## 贝叶斯模型

>  介绍

朴素贝叶斯模型是基于贝叶斯定理与特征条件独立假设的分类器。

> 模型
>
> 假设

在研究朴素贝叶斯模型在一个分类问题的应用时，先对数据进行这样的假设：输入空间$ D_X\subset \R^d$是由训练数据集中所有$d$维的特征向量$x\in X$组成的，输出空间$D_Y\subset \R$始是由每个训练样本$x$的所属类别的标号组成的，假设总共有$K$个类别，即$D_Y=\{c_1,c_2,...c_K\}$. 训练数据集$D=\{(x_i,y_i)\}_{1\leq i \leq N}$. 同时假设每个数据$(x_i,y_i)$是独立同分布的，满足联合概率分布$P(X,Y)$.

> 推导

首先，我们定义损失函数：
$$
L(Y,f(X))=\begin{cases} 0,Y\neq f(x) \\1,Y =f(x) \end{cases} \tag{1}
$$
先验概率为
$$
P(Y=c_k)\\i=1,2,...K
$$




## EM算法

**算法问题引入**

假设有这样一个关于概率的问题：有三个不均匀构造的硬币，被抛出后都分别只有正面和反面两种结果，第一个硬币出现正面的概率为$a$，第二个硬币出现正面的概率为$b$，第三个硬币出现正面的概率为$c$。现在进行如下的实验：先抛出硬币1，如果硬币1结果是正面，就再次抛出硬币2，最后记录结果；如果硬币1结果是反面，就再次抛出硬币3，最后记录结果。但是最后我们只知道记录的结果{0,1}，不知道这个结果是来自硬币2还是硬币3，最后我们需要根据这些参数和样本来估计之后观测数据出现的结果。整个过程是这样的$X \text{硬币1} \rightarrow Y \text{硬币2或3} \rightarrow Z \text{观测}$.

让我们来求解这个特例的模型：

观测$Z=1$的概率为：
$$
P(Z=1)=a
$$



