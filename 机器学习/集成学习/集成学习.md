## 集成学习

提升方法（boosting method）在机器学习应用中非常常见，在工程中遇到的绝大多数问题都是具有很高的复杂性的，如特征维度非常高，数据的量非常大。提升方法启发我们运用多个回归器或者分类器的组合来完成很高复杂的工作，并且在实际应用过程中非常高效和精准。

本章节主要介绍提升方法中非常经典的AdaBoost算法，其中包括AdaBoost算法的思想，算法的原理和推导，算法的有效性证明，并且给出算法的基本框架；然后基于AdaBoost的方法来对几个较复杂的数据集进行分类和回归学习，同时与普通方法进行对比。AdaBoost是算法是由科学家Freund和Schapire（1995）提出的。

### 集成学习与提升方法的思想

我们之前所研究是单一的学习方法，那么我们先思考这样一个问题：如果存在一个复杂的任务，以至于大多数的单一学习方法不能够很好地作出综合的判断和预测，那么是否可以将很多个不同的或者相同的学习方法组合起来，来提高算法的学习能力和预测精度？

这就是集成学习和提升方法被提出的动机，Valiant & Kearns曾经提出：在概率近似正确（PAC）的学习框架中，一个概念（类），如果存在一个多项式的学习算法可以学习并且具有很高的正确率，就称这个概念强可学习（strongly learnable）；对一个概念（类），如果存在一个多项式的学习算法可以学习，并且正确率并不是很高但不比完全随机的猜测差，就称这个概念弱可学习（weakly learnable）. 后来被 Schapire 证明：一个概念存在强学习的算法的必要条件是这个概念必须是存在弱学习算法。于是，对于一个概念，如何将一个或者一些已知的弱学习算法转化提升成强学习算法成为了一个重要的研究方向。目前针对这种问题有很多的主流解决方法，其中AdaBoost就是非常重要的一个.

集成学习的主要结构如下图【图1】. 可以考虑将相同的学习器进行组合，即同质集成（homogeneous ensemble）也可以考虑将不同的学习器进行组合，即异质集成（heterogeneous ensemble）. 通常情况下，集成学习器比单一的个体学习器在训练误差上和泛化性能上更优，但是并不是说明一定会在性能上有提升，甚至某些情况下，不适合的学习器组合起来有可能会导致整体性能的下降。

![1539915008794](C:\Users\lenovo\Desktop\写作\机器学习\4 决策树\1539915008794.png)

考虑一个比较简单的例子：一个二分类问题，学习函数$f:X\rightarrow Y$，采用投票法训练$K$个同质分类器，对每个分类器$h_i$假设存在一个误差
$$
P(h_i(x)\neq y)=\epsilon \tag{1}
$$
最终样本$x$被分类为$K$个分类器结果中比例最大的那个类，模型可以等价表达为
$$
f(x)=sign(\sum_{i=1}^K{h_i(x)}) \tag{2}
$$
那么集成分类器误差可以表示为
$$
P(Y\neq f(X))=\sum_{i=1}^{\lfloor{K/2}\rfloor}{ \binom{K}{i} (1-\epsilon)^i\epsilon^{K-i}}\tag{3}
$$
根据之前的hoeffding不等式，可以得到
$$
P(Y\neq f(X))\leq exp(-\frac{1}{2}K(1-2\epsilon)^2)\tag{4}
$$
于是我们得到了整个集成分类器的一个误差上界，根据$()$可以看出，当组成同质集成分类器的个体分类器的数量越大，误差上界就会以指数形式减小，最终整个分类器的误差会在零处收敛。

对于集成学习算法主要关注两个问题：一每个个体学习器应该以什么样的结构组合成一个强学习器；二每一次学习后应该如何根据误差来指导算法完成调整。集成学习对个体学习器的组成结构提出了很多不同多样的想法，其中包括最常用的树形结构模型和加法模型（AdaBoost就是其中一种）；同时集成学习一般会在每一次的迭代过程中重新调整训练数据的权值，即重新调整训练数据的分布，这样的目的是为了让当前训练误差较大的样本在之后的迭代过程中能被学习器更多地关注，从而强化了整个集成学习器的能力。





AdaBoost算法是一个广义加法模型的特例，我们先来研究广义的加法模型.





### AdaBoost算法原理和推导

AdaBoost是广义加法模型的一个特例，也是集成学习算法中比较经典和常用的一种。对于集成学习算法所关注的两个主要问题，AdaBoost提出了这样的解决方案：一采用加权投票的方式来组合各个弱学习器，在最终求得的模型中对于误差较大的学习器自然具有较大的权值，而误差较小的学习器自然对最终的结果决定权较小；二AdaBoost在每一轮的迭代过程中会根据每个个体样本的误差情况来调整每个样本所占的权值，如果当前样本的训练误差较大，就适当增加它的权值，使得它在之后的迭代过程中能更多地被学习器所关注，从而学习器能作出适当和及时的调整，如果训练误差较小，就减少它的权值。

我们就二分类问题对AdaBoost进行分析：

存在这样的训练数据集$D=\{(x_i,y_i)\}_{1\leq i\leq N}$ 一共包括N个样本其中$y_i=\{-1,1\}$，我们要学习一个真实函数$f:X\rightarrow Y$.

根据AdaBoost算法的思想，最终求解的由K个弱分类器组合的强分类器模型具有这样的形式
$$
h(x)=\sum_{i=1}^K{\alpha_iG_i(x)} \tag{5}
$$
其中$G_i(x)$代表第$i$个个体分类器，$\alpha_i$是对个体分类器的权值，满足$\alpha_i\geq 0$，但是$\sum_{i=1}^K{\alpha_i}$不一定是0. 这是对所有个体分类器的线性组合成为强分类器. 

对于这个二分类问题，式$()$可以直接根据投票法给出最终的分类结果
$$
G(x)=sign(h(x))=sign(\sum_{i=1}^K{\alpha_iG_i(x)})\tag{6}
$$
然后给出AdaBoost的指数损失函数，强分类器的参数就按照损失函数最小化原则进行求解
$$
L(h\;|\;\mathcal{D})=E_{x}(\exp(-yh(x))\;|\;x\sim \mathcal{D})\tag{7}
$$
其中$\mathcal{D}$是当前样本的分布，这个分布按照AdaBoost算法每一次迭代是可调节的，式$(7)$是对所有样本$x$, $h(x)$的指数损失在总体样本满足分布$\mathcal{D}$时，的均值，即关于变量$x$的平均损失. 

接下来的任务就是求解$(6)$和$(7)$式，从中得到参数$\alpha_i$和分类器$G_i(x)$的表示形式. 在推导之前，有必要了解关于广义加法器模型的推导方式，因为考虑到AdaBoost是广义加法模型的特例，那么是否广义加法模型的推导方法也对AdaBoost算法适用？

### 广义加法模型与前向分步算法

对于一个广义的加法模型，一般具有这样的形式
$$
f(x)=\sum_{i=1}^K{\beta_iB(x,\gamma_i)}\tag{8}
$$
其中，$\beta_i$是一个权值，和AdaBoost中的$\alpha_i$具有相同的意义，$B(\cdot)$是一个基函数，$\gamma_i$是基函数的未知参数. 可以看出$(5)$和$(8)$具有相同的形式.

在一个特定的学习任务下，给定训练数据集和损失函数的形式$L(y,f(x))$的条件下，学习加法模型$(8)$就是一个损失函数最小化的优化问题
$$
\min_{\beta_k,\gamma_k}{\sum_{i=1}^N{L\left(y_i,\sum_{k=1}^K{\beta_kB(x_i,\gamma_k)}\right)}}\tag{9}
$$
关于如何求解式$(9)​$有一个比较简单和常用的方法，前向分步算法. 这个算法的基本思想是：加法模型中误差是从前往后传递的，即后一个学习器是为了减小前一个学习器带来的误差，因此可以以考虑从前往后逐个学习. 这样问题就化简为了学习第$k​$个分类器
$$
\min_{\beta_k,\gamma_k}{\sum_{i=1}^N{L\left(y_i,{\beta_kB(x_i,\gamma_k)}\right)}} \tag{10} \\
k=1,2,...K
$$
也就是从$k=1$开始求解关于$\beta_k,\gamma_k$的子问题，一直到$k=K$结束. 求得的集成学习器是满足式$(9)$的要求的.

将前向分步算法化简成算法框架

输入：训练数据集$D=\{(x_i,y_i)\}_{1\leq i\leq N}$；定义损失函数$L(y,f(x))$，定义基函数$\{B_k(x,\gamma_k)\}_{1\leq k\leq K}$.

输出：最终的集成学习器$f(x)$.

1. 初始化$f_0(x)=0$
2. 令$k=1:M$

求解第k个损失函数最优化问题
$$
(\beta_k,\gamma_k)=\arg_{\beta,\gamma}\min{\sum_{i=1}^N{L\left(y_i,f_{k-1}(x_i)+\beta B_k(x_i,\gamma) \right)}}\tag{11}
$$
求解递推式
$$
f_k(x)=f_{k-1}(x)+\beta_kB_k(x,\gamma_k)\tag{12}
$$

3. 循环结束，得到最终的模型

$$
f(x)=f_K(x)\tag{13}
$$

从算法框架中可以看出，前向分步算法实质上是利用第k个学习器来最小化前k-1个学习器的训练误差，然后继续向第k+1个学习器执行相同的操作，将复杂的优化问题$(9)$简化成了一般的优化问题.

### AdaBoost原理和推导

那么我们是否能够利用广义加法模型的前向分步算法来推导AdaBoost算法呢？接下来要说明的就是AdaBoost算法实质上是损失函数定义为指数函数的前向分步算法，因为它利用了前向分步算法作为第k轮迭代的假设. AdaBoost基本可以分成三个步骤，一学习当前的学习器$G(x)$；二对数据分布的权值进行调整；三计算参数$\alpha$的值. AdaBoost算法的流程基本是这样的：先根据之前调整的新的样本分布$\mathcal{D_k}$来学习当前第k次迭代的学习器$G_k$和新的样本分布$\mathcal{D_{k+1}}$，在理论上$G_k$应该能完全减少之前k的-1个集成学习器的误差。



当$k=1$时，数据分布$\mathcal{D_1}$就是训练数据的原始分布$\mathcal{D}$，其中$\mathcal{D_1}=(\omega_{11},...\omega_{1N})$，初始情况下$\omega_{1i}=\frac{1}{N},i=1,...N$.

当$k\geq1$时，此时的训练数据分布$\mathcal{D_k}$是由第k-1轮迭代计算得到的，既然分布$\mathcal{D_k}$ 已经求得，那么就应该计算在该分布下的参数$\alpha_k$，AdaBoost假设利用第k个学习器理论上能够完全纠正或者至少减少前k-1个学习器的误差，那么第k轮集成学习器表示为
$$
h_k(x)=h_{k-1}(x)+\alpha_kG_k(x)\tag{14}
$$
第k轮迭代的目标是按照前面k-1个学习器的集成来学习第k个学习器$G_k$，即假设结合成的集成学习器$h_{k-1}+G_k$在原始样本分布$\mathcal{D}$下是能够满足损失最小化原则的；求出$G_k$后调整分布$\mathcal{D_{k}}$为$\mathcal{D_{k+1}}$，并求出$G_k$的权重$\alpha_k$.
$$
L(h_{k-1}+G_k\;|\;\mathcal{D})\\
=E_{x}\left[\;\exp(-y\;(h_{k-1}(x)+G_k(x)\;))\;|\;\mathcal{D}\;\right]\\
=E_x\left[\;\exp(-yh_{k-1}(x))\exp(-yG_k(x))\;|\;\mathcal{D}\;\right]\tag{15}
$$
该损失函数的意义是：在$x$满足原来训练数据的分布$\mathcal{D}$情况下，目前从第1个个体学习器一直到第k个个体学习器的集成学习器必须使得将$x$映射后的分类误差最小. 

//事实上根据前向分步算法，式$(15)$转换成最优化问题就等价于如下常见的形式：
$$
\begin{align}
(\alpha_k,G_k)&=\arg_{\alpha,G}\min\sum_{i=1}^N{\exp[-y_i(h_{k-1}(x_i)+\alpha G(x_i))]}\\
&=\arg_{\alpha,G}\min\sum_{i=1}^N{\Omega_{k,i}\exp[-y_i\alpha G(x_i)]}
\end{align}
$$
其中$\Omega_{k,i}=\exp[-y_ih_{k-1}(x_i)]$ 与$\alpha,G$无关的系数. 

由式$(15)$可以看出参数$\alpha_k$只和第二项有关，根据损失函数最小化的原则令$(15)$对参数$\alpha_k$求导等于0，其中第一项可以看作是常数于是对第二项进行研究.





先关注权值$\alpha_k$的更新，因为它在数学形式上更加简单和清晰，此时的前提条件是最优的分类器$G_k$已经求得；接下来需要赋予学习器$G_k$一个合适的权值，当然赋予权值的目标是
$$
\min{L(\alpha_k G_k\;|\; \mathcal{D_k})}\tag{}
$$
注意此时的赋权学习器$\alpha_k G_k$是按照样本分布$\mathcal{D_k}$进行经验最小化风险求解的。进一步有
$$
L(\alpha_k G_k\;|\; \mathcal{D_k})\\
=E_x[\exp(-yG_k(x))\;|\;\mathcal{D_k}\;]\\
=E_x[e^{-\alpha_k}\mathbb I(y=G_k(x)\;)+e^{\alpha_k}\mathbb I(y\neq G_k(x)\;)\;|\;\mathcal{D_k}\;]\tag{16}
$$
其中$\mathbb I(\cdot)$为指示函数，如果为TRUE输出1，如果为FALSE输出0. 因为$G_k(x)$只有等于和不等于$y$两种情况，所以能将乘积拆成指示函数和的形式：如果$y=G_k(x)$，那么$yG_k(x)=1$；如果$y\neq G_k(x)$，那么$yG_k(x)=-1$. 这里再做一个说明：如果我们在对$x$求期望时关注的是$G_k$这个部分而不是$h_{k-1}+G_k$的集成学习器，那么这个时候对学习器$G_k$而言，关于学习的数据$x$的分布是由前面k-1个学习器修改过的，此时$x\sim \mathcal{D_k}$而不是原始分布$x\sim \mathcal{D}$了.

同时因为$(16)$是对$x$求期望的，前面不包括$x$的乘积可以看作是常数，因此可以进一步化简为
$$
\begin{align}
&E_x[\exp(-y\alpha_kG_k(x))\;|\;\mathcal{D_k}\;]\\
&=e^{-\alpha_k}P_x(y=G_k(x))+e^{\alpha_k}P_x(y\neq G_k(x))\\
&=e^{-\alpha_k}(1-\epsilon_k)+e^{\alpha_k}\epsilon_k
\end{align}\tag{17}
$$
其中$\epsilon_k=P_x(y\neq G_k(x))$. 对指示函数的$x$期望实质上就是等于关于$x$概率，因为指示函数只有当满足条件时才为1. 令$(17)$对$\alpha_k$求导等于0得到
$$
\frac{\part{L}}{\part{\alpha_k}}=e^{\alpha_k}\epsilon_k-e^{-\alpha_k}(1-\epsilon_k)=0\tag{18}
$$
最终求得参数$\alpha_k$的值
$$
\alpha_k=\frac{1}{2}\ln\left(\frac{1-\epsilon_k}{\epsilon_k}\right)\tag{19}
$$

由$(19)$可以看出，最终的$\alpha_k$只和当前的学习器$G_k$的错误率有关，和之前的所有学习器是没有关系的. 如果当前学习器的误差较小，那么当前学习器就越有效，$\alpha_k$就会更大，偏向于提高学习器的重要性；如果当前学习器的误差较大，那么就会占更小的比重. 想要求出$\alpha_k$必须首先要知道学习器$G_k$，事实上因为关于$\alpha_k$的推导比较简单，因此先被讲解，在实际求解过程中应该先计算$G_k$，这里为了简便就暂时换一下顺序，具体的计算顺序和逻辑关系在下一节的算法框架可以看得很清晰.

之前的式$(19)$是基于学习器$G_k$已经被求得，学习最优分类器$G_k$是基于目标：习得的$G_k$在理论上总能够使得之前的集成学习器$h_{k-1}​$的全部误差被纠正。根据最小化损失函数
$$
\min_{ G_k} L(h_{k-1}+G_k\;|\;\mathcal{D})\\
=\min_{G_k}{E_x[\exp(-y(h_{k-1}(x)+G_k(x)))\;|\;\mathcal{D}]}\\
=\min_{ G_k} E_x\left[\;\exp(-yh_{k-1}(x))\exp(-yG_k(x))\;|\;\mathcal{D}\;\right]\tag{20}
$$
在理想情形下，$y\in\{-1,+1\}$且$G_k(x)\in\{-1,+1\}$，那么$y^2=G_k(x)^2=1$. 下面对式$(20)$含$G_k$的一项进行泰勒展开
$$
e^{-yG_k(x)}=\sum_{i=0}^{n}{\frac{(-1)^i}{i!}[yG_k(x)]^i}\tag{}
$$
其中展开的余项表达式为
$$
\frac{(-1)^n}{n!}[yG_k(x)]^n\leq \frac{1}{n!} \tag{}
$$
因为级数的第n项是趋于零的，即
$$
\lim_{n\rightarrow+\infty}{\frac{1}{n!}}=0
$$
所以泰勒展开的余项也是随着n的增大趋于零的。

根据泰勒展开，我们对第二项$\exp$取二阶近似式
$$
\begin{align}
L(h_{k-1}+G_k\;|\;\mathcal{D})&\simeq E_x\left[\exp(-yh_{k-1}(x))\;\left(1-yG_k(x)+\frac{yG_k(x)}{2}\right)\;|\;\mathcal{D} \right]\\
&=E_x\left[\exp(-yh_{k-1}(x))\;\left(1-yG_k(x)+\frac{1}{2}\right) \;|\;\mathcal{D}\right]
\end{align}\tag{21}
$$
在上式中，因为研究的是整个集成分类器，$x$的分布仍然是取$\mathcal{D}$. 

下面进行等价变形的推导
$$
\begin{align}
&\min_{G_k}{E_x\left[\exp(-yh_{k-1}(x))\;\left(1-yG_k(x)+\frac{1}{2}\right) \;|\;\mathcal{D}\right]}
\\&=\max_{G_k}{E_x\left[ \exp(-yh_{k-1}(x))\;yG_k(x)\;|\;\mathcal{D}\right]}

\\&=\max_{G_k}{E_x
\left[
\frac{e^{-yh_{k-1}(x)}}{E_x\left[e^{-yh_{k-1}(x)}\;|\;\mathcal{D}\right]}
yG_k(x)\;|\; \mathcal{D}
\right]}
\end{align}\tag{22}
$$
第一行到第二行是去除掉了不包含$G_k(x)$的常数项；第二行到第三行是对前面的一项进行了转化，其目的是凑成新的分布$\mathcal{D_{k+1}}$.

根据期望的性质，假设随机变量$X\sim D$，其中任意一个$x$的概率为$p(x)$，$f$为非负连续函数，则
$$
\frac{f(x)}{E[f(x)]}p(x)=\frac{p(x)f(x)}{\int_{\mathcal{X}} p(x)f(x)dx}\in[0,1]\tag{23}
$$
并且有
$$
\int_{\mathcal{X}}\frac{p(x)f(x)}{\int_{\mathcal{X}} p(x)f(x)dx}dx=1\tag{24}
$$
因此$\frac{p(x)f(x)}{\int_{\mathcal{X}} p(x)f(x)dx}$是一个新的概率分布。

于是按照式$(22)$可以得到一个新的分布$\mathcal{D_{k}}$，其中
$$
\mathcal{D_{k}(x)}=\frac{e^{-yh_{k-1}(x)}}{E_x\left[e^{-yh_{k-1}(x)}\;|\;\mathcal{D}\right]}\mathcal{D(x)}\tag{25}
$$
其中式$(25)$中的前一部分是依赖于k的。

根据式$(22)$和式$(25)$可以得到关于第k轮迭代的学习器$G_k$
$$
G_k=\arg_G\max{E_x[yG_(x)\;|\;\mathcal{D_k}]}\tag{26}
$$
根据$y\in\{-1,+1\}$且$G_k(x)\in\{-1,+1\}$可以得到
$$
yG(x)=+1 \;\text{,if}\;y= G(x)
\\yG(x)=-1 \;\text{,if}\;y\neq G(x)\tag{27}
$$
可以利用上述性质对式$(26)$进行化简
$$
yG(x)=2 \mathbb I(y =G(x))-1\tag{28}
$$
最后的第k轮最优分类器模型为
$$
G_k(x)=\arg_{G} \min(E_x[\mathbb I(y\neq G(x))\;|\; \mathcal{D_k}])\tag{29}
$$

得到的最后结果式$(29)$十分直观，分类器$G_k$是最优的就等价于$G_k$应该满足在新的样本分布$\mathcal{D_k}$分类正确率是最高的。

同时我们以根据式$(25)$得到第k轮迭代的样本分布更新公式
$$
\begin{align}
\mathcal{D_{k+1}}(x) &=\frac{e^{-yh_k(x)}}{E_x[e^{-yh_k(x)}\;|\;\mathcal{D}]}\mathcal{D}(x)\\
&=\frac{e^{-yh_{k-1}(x)}e^{-y \alpha_k G_k(x)}}{E_x[e^{-yh_k(x)}\;|\;\mathcal{D}]}\mathcal{D}(x)\\
&=\frac{E_x\left[e^{-yh_{k-1}(x)}\;|\;\mathcal{D}\right]e^{-y \alpha_k G_k(x)}}{E_x[e^{-yh_k(x)}\;|\;\mathcal{D}]}\cdot
\frac{e^{-yh_{k-1}(x)}}{E_x\left[e^{-yh_{k-1}(x)}\;|\;\mathcal{D}\right]}\mathcal{D(x)}
\\
&=\frac{E_x\left[e^{-yh_{k-1}(x)}\;|\;\mathcal{D}\right]}{E_x[e^{-yh_k(x)}\;|\;\mathcal{D}]}
\cdot e^{-y \alpha_k G_k(x)} \cdot \mathcal{D_k}(x)
\end{align}\tag{30}
$$
关于样本分布更新，最后得到的是一个递推式，式$(30)$的推导过程只是关于k的简单递推，同时也可以看出分布$\mathcal{D_{k+1}}$的计算也是依赖于分类器$G_k​$的误差的。



### AdaBoost算法框架

以上关于AdaBoost算法的推导完成，具体的算法框架整理如下：

给定一个二分类问题，数据集$D=\{(x_i,y_i)\}_{1\leq i \leq N}$，以及对样本空间的限制$\mathcal{Y}\in\{+1,-1\}$，$\mathcal{X}\subset\R^d$.

下面利用AdaBoost算法学习一个集成分类器$G=\sum_{i=1}^M{\alpha_k G_k(x)}$，使得$G:\mathcal{X}\rightarrow\mathcal{Y}$.



输入：训练数据$D=\{(x_i,y_i)\}_{1\leq i \leq N}$，其中$\mathcal{Y}\in\{+1,-1\}$，$\mathcal{X}\subset\R^d$，弱分类器$G_k$

输出：集成分类器$G$.

1. 初始化：

样本权值$\mathcal{D_1}=\{\omega_{1,i}\}_{1 \leq i\leq N}$，其中$\omega_{1,i}=\frac{1}{N}$

2.  采用前向分步算法，在第k轮迭代学习分类器$G_k$

for k=1:M

首先，基于样本分布$\mathcal{D_k}$权值，学习弱分类器$G_k:\mathcal{X}\rightarrow \mathcal{Y}$，其中$X\sim \mathcal{D_k}$

然后，计算分类器$G_k$的损失率
$$
\epsilon_k=P_x(y\neq G_k(x))=\sum_{i=1}^N{\mathbb I(y_i\neq G_k(x_i))}\tag{31}
$$
接着，得到第k个分类器的权值
$$
\alpha_k=\frac{1}{2}\ln\left(\frac{1-\epsilon_k}{\epsilon_k}\right)
$$
最后更新样本的分布权值，根据式$(30)$可以得到
$$
\omega_{k+1,i0}=\omega_{k,i}\frac{e^{-\alpha_k y_i G_k(x_i)}}{\sum_{i=1}^N{\omega_{k,i}e^{-\alpha_k y_i G_k(x_i)}}}\tag{32}
$$
其中第k+1轮的样本分布权值为
$$
\mathcal{D_{k+1}}=(\omega_{k+1,1},\omega_{k+1,2},...\omega_{k+1,N})\tag{33}
$$
end for

3. 迭代结束后，得到的最终线性集成分类器

$$
f(x)=\sum_{k=1}^M{\alpha_k G_k(x)}\tag{34}
$$

样本根据模型得到的分类为
$$
G(x)=\mathbb I(f(x))\tag{35}
$$
其中需要说明的是，如果一个弱分类器的分类误差大于0.5，也就是说明比随机猜测还要差的话，该弱分类器会被直接舍弃，否则会影响整个集成分类器的性能。

### 计算实例

### AdaBoost算法计算实例



### *关于AdaBoost算法的证明

AdaBoost在学习的过程，是利用前向分布算法逐一组合弱分类器的过程，下面需要证明在此过程中训练误差是不断减少的，同时可以收敛于零。

首先，需要先证明

 

### 随机森林

 